# Location of the datasets used in this project


## SuperGlue tasks

* [Ax-b](https://huggingface.co/datasets/stjokerli/TextToText_axb_seqio)
* [Ax-g](https://huggingface.co/datasets/stjokerli/TextToText_axg_seqio)
* [CB](https://huggingface.co/datasets/stjokerli/TextToText_cb_seqio)
* [BOOLQ](https://huggingface.co/datasets/stjokerli/TextToText_boolq_seqio)
* [RTE](https://huggingface.co/datasets/stjokerli/TextToText_rte_seqio)
* [WIC](https://huggingface.co/datasets/stjokerli/TextToText_wic_seqio)
* [WSC](https://huggingface.co/datasets/stjokerli/TextToText_wsc_seqio)
* [ReCorD](https://huggingface.co/datasets/stjokerli/TextToText_record_seqio)
* [multiRC](https://huggingface.co/datasets/stjokerli/TextToText_multiRC_seqio)

Note that `RTE` training and validation sets are added to the `Ax-b` and `Ax-g` for easy usage

## Source tasks

* [MNLI](https://huggingface.co/datasets/stjokerli/TextToText_mnli_seqio)
* [SQUAD](https://huggingface.co/datasets/stjokerli/TextToText_squad_seqio)

## Additional Tasks. 

* [DocNLI](https://huggingface.co/datasets/stjokerli/TextToText_DocNLI_seqio)


## Attribution

For consistency, we have based our code off the implementations from the following references:
- Lester et al. [ref] (https://aclanthology.org/2021.emnlp-main.243/)
- Vu et al. [ref] (https://arxiv.org/abs/2110.07904) (to be published in ACL 2022)
See the official prompt tuning github repository [here](https://github.com/google-research/prompt-tuning)
