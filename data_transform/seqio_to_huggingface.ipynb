{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3a5d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqio\n",
    "# !pip install t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82eb19ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U huggingface_hub\n",
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8490ba62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqio\n",
    "import functools\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import t5.data\n",
    "from t5.data import postprocessors\n",
    "from t5.data import preprocessors\n",
    "from t5.data.glue_utils import get_glue_metric\n",
    "from t5.data.glue_utils import get_glue_postprocess_fn\n",
    "from t5.data.glue_utils import get_glue_text_preprocessor\n",
    "from t5.data.glue_utils import get_super_glue_metric\n",
    "from t5.evaluation import metrics\n",
    "import tensorflow_datasets as tfds\n",
    "from t5.models import utils as model_utils\n",
    "import gin\n",
    "from absl import logging\n",
    "from datasets import load_dataset,concatenate_datasets,DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "772042f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.2+nightly'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d703d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seqio.dataset_providers.Task at 0x7fb3d2742940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskRegistry = seqio.TaskRegistry\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True,\n",
    "        required=False),\n",
    "    \"targets\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True)\n",
    "}\n",
    "\n",
    "# ==================================== C4 ======================================\n",
    "# Final pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_span_corruption\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.span_corruption,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Baseline pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_iid_denoising\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.iid_denoising,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Prefix language modeling pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_prefix_lm\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.prefix_lm,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Configurable tasks used for comparisons in Raffel et al., 2019.\n",
    "_c4_config_suffixes = [\"\", \".noclean\", \".realnewslike\", \".webtextlike\"]\n",
    "for config_suffix in _c4_config_suffixes:\n",
    "    TaskRegistry.add(\n",
    "        \"c4{name}_v020_unsupervised\".format(name=config_suffix.replace(\".\", \"_\")),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"c4/en{config}:2.2.0\".format(\n",
    "          config=config_suffix)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.rekey, key_map={\n",
    "                  \"inputs\": None,\n",
    "                  \"targets\": \"text\"\n",
    "              }),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          preprocessors.unsupervised,\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        metric_fns=[])\n",
    "\n",
    "\n",
    "# ================================ Wikipedia ===================================\n",
    "TaskRegistry.add(\n",
    "    \"wikipedia_20190301.en_v003_unsupervised\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wikipedia/20190301.en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.unsupervised,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# =================================== GLUE =====================================\n",
    "for b in tfds.text.glue.Glue.builder_configs.values():\n",
    "    TaskRegistry.add(\n",
    "        \"glue_%s_v002\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/%s:2.0.0\" % b.name,\n",
    "        splits=[\"test\"] if b.name == \"ax\" else None),\n",
    "        preprocessors=[\n",
    "        get_glue_text_preprocessor(b),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=get_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "# =============================== CNN DailyMail ================================\n",
    "TaskRegistry.add(\n",
    "    \"cnn_dailymail_v002\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"cnn_dailymail:3.1.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.summarize,\n",
    "            article_key=\"article\",\n",
    "            summary_key=\"highlights\"),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.rouge],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ==================================== WMT =====================================\n",
    "# Format: year, tfds builder config, tfds version\n",
    "b_configs = [\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"16\", tfds.translate.wmt16.Wmt16Translate.builder_configs[\"ro-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"15\", tfds.translate.wmt15.Wmt15Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"19\", tfds.translate.wmt19.Wmt19Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for prefix, b, tfds_version in b_configs:\n",
    "    TaskRegistry.add(\n",
    "        \"wmt%s_%s%s_v003\" % (prefix, b.language_pair[1], b.language_pair[0]),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"wmt%s_translate/%s:%s\" %\n",
    "                                  (prefix, b.name, tfds_version)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.translate,\n",
    "              source_language=b.language_pair[1],\n",
    "              target_language=b.language_pair[0],\n",
    "          ),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=[metrics.bleu],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Special case for t2t ende.\n",
    "b = tfds.translate.wmt_t2t.WmtT2tTranslate.builder_configs[\"de-en\"]\n",
    "TaskRegistry.add(\n",
    "    \"wmt_t2t_ende_v003\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wmt_t2t_translate/de-en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.translate,\n",
    "            source_language=b.language_pair[1],\n",
    "            target_language=b.language_pair[0]),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.bleu],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= SuperGlue ==================================\n",
    "for b in tfds.text.super_glue.SuperGlue.builder_configs.values():\n",
    "  # We use a simplified version of WSC, defined below\n",
    "    if \"wsc\" in b.name:\n",
    "        continue\n",
    "    if b.name == \"axb\":\n",
    "        glue_preprocessors = [\n",
    "            functools.partial(\n",
    "                preprocessors.rekey,\n",
    "                key_map={\n",
    "                    \"premise\": \"sentence1\",\n",
    "                    \"hypothesis\": \"sentence2\",\n",
    "                    \"label\": \"label\",\n",
    "                    \"idx\": \"idx\",\n",
    "                }),\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "        ]\n",
    "    else:\n",
    "        glue_preprocessors = [\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "    ]\n",
    "    TaskRegistry.add(\n",
    "        \"super_glue_%s_v102\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "          tfds_name=\"super_glue/%s:1.0.2\" % b.name,\n",
    "          splits=[\"test\"] if b.name in [\"axb\", \"axg\"] else None),\n",
    "        preprocessors=glue_preprocessors,\n",
    "        metric_fns=get_super_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "    # Create SuperGLUE tasks with 1 sentinel token added.\n",
    "    seqio.experimental.add_task_with_sentinels(\"super_glue_%s_v102\" % b.name,\n",
    "                                             num_sentinels=1)\n",
    "\n",
    "# ======================== Definite Pronoun Resolution =========================\n",
    "TaskRegistry.add(\n",
    "    \"dpr_v001_simple\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"definite_pronoun_resolution:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.definite_pronoun_resolution_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"dpr_v001_simple\", num_sentinels=1)\n",
    "\n",
    "# =================================== WSC ======================================\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_train\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"train\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.wsc_simple, correct_referent_only=True),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_train\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.wsc_simple, correct_referent_only=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_eval\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "# =================================== WNLI =====================================\n",
    "TaskRegistry.add(\n",
    "    \"glue_wnli_v002_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/wnli:1.0.0\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        preprocessors.wnli_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# =================================== Squad ====================================\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_context_free\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.squad, include_context=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Squad span prediction task instead of text.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers_span\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad_span_space_tokenized,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.span_qa,\n",
    "    metric_fns=[metrics.span_squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Deprecated: Use `squad_v010_allanswers` instead.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= TriviaQA ===================================\n",
    "TaskRegistry.add(\n",
    "    \"trivia_qa_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"trivia_qa/rc:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.trivia_qa,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.trivia_qa_truncate_inputs,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# =============== PrefixLM objectives (not used in the T5 paper) ===============\n",
    "\n",
    "\n",
    "# # Vocabulary (shared by encoder and decoder)\n",
    "# sentencepiece_model_file = \"gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model\"\n",
    "\n",
    "# vocab = seqio.SentencePieceVocabulary(sentencepiece_model_file)\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_encoder_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_encoder_decoder,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"encoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_decoder_only,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_causal_attention\": seqio.Feature(\n",
    "#             vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"c4_v220_full_lm\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.full_lm,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, add_eos=True)\n",
    "#     },\n",
    "#     metric_fns=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e69ad3",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b929488e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"super_glue_axg_v102\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"test\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cbf0a28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['idx', 'inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets'])\n",
      "{'idx': 104, 'inputs_pretokenized': b\"rte hypothesis: The surgeon's affection had grown over time. premise: The surgeon operated on the child with great care; his affection had grown over time.\", 'inputs': array([    3,    52,    17,    15, 22455,    10,    37, 12177,    31,\n",
      "           7, 18615,   141,  4503,   147,    97,     5,     3, 17398,\n",
      "          10,    37, 12177,  7747,    30,     8,   861,    28,   248,\n",
      "         124,   117,   112, 18615,   141,  4503,   147,    97,     5,\n",
      "           1], dtype=int32), 'targets_pretokenized': b'entailment', 'targets': array([   3,   35, 5756,  297,    1], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "for i, ex in enumerate(iterator):\n",
    "    print(ex.keys())\n",
    "    print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05056184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: John hired Bill to take care of *him* .', 'inputs': array([    3,   210,     7,    75,    10,  1079, 10626,  3259,    12,\n",
      "         240,   124,    13,  1429, 10813,  1935,     3,     5,     1],\n",
      "      dtype=int32), 'targets_pretokenized': b'John', 'targets': array([1079,    1], dtype=int32), 'label': 1, 'idx': 461}\n"
     ]
    }
   ],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"super_glue_wsc_axg_n\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "for i, ex in enumerate(iterator):\n",
    "    print(ex.keys())\n",
    "    print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47efe2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cde53ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"idx\": [],\n",
    "    \"inputs_pretokenized\": [],\n",
    "    \"targets_pretokenized\": []\n",
    "}\n",
    "for i, ex in enumerate(iterator):\n",
    "    dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "    dictionary[\"inputs_pretokenized\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\") )\n",
    "    dictionary[\"targets_pretokenized\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69eac24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cec6d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idx', 'inputs_pretokenized', 'targets_pretokenized']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "018329b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"mnli hypothesis: You won't learn anything by serving overseas.  premise: because actually when you when you do uh service overseas you end up learning something usually that's that's really useful plumbing or farming or or something like that so you're really learning a skill\",\n",
       " 'mnli hypothesis: This is a stringed instrument created by the ancient Aztecs. premise: The geiro, a percussion instrument made of a notched dried gourd, was developed by the Taano Indians.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['inputs_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1906ba9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contradiction', 'contradiction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['targets_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6e6c95bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No Task or Mixture found with name 'glue_wic_v002'. Available:\n - c4_noclean_v020_unsupervised\n - c4_realnewslike_v020_unsupervised\n - c4_v020_unsupervised\n - c4_v220_iid_denoising\n - c4_v220_prefix_lm\n - c4_v220_span_corruption\n - c4_webtextlike_v020_unsupervised\n - cnn_dailymail_v002\n - dpr_v001_simple\n - dpr_v001_simple_1_sentinel\n - glue_ax_v002\n - glue_cola_v002\n - glue_mnli_matched_v002\n - glue_mnli_mismatched_v002\n - glue_mnli_v002\n - glue_mrpc_v002\n - glue_qnli_v002\n - glue_qqp_v002\n - glue_rte_v002\n - glue_sst2_v002\n - glue_stsb_v002\n - glue_wnli_v002\n - glue_wnli_v002_simple_eval\n - squad_v010\n - squad_v010_allanswers\n - squad_v010_allanswers_span\n - squad_v010_context_free\n - super_glue_axb_v102\n - super_glue_axb_v102_1_sentinel\n - super_glue_axg_v102\n - super_glue_axg_v102_1_sentinel\n - super_glue_boolq_v102\n - super_glue_boolq_v102_1_sentinel\n - super_glue_cb_v102\n - super_glue_cb_v102_1_sentinel\n - super_glue_copa_v102\n - super_glue_copa_v102_1_sentinel\n - super_glue_multirc_v102\n - super_glue_multirc_v102_1_sentinel\n - super_glue_record_v102\n - super_glue_record_v102_1_sentinel\n - super_glue_rte_v102\n - super_glue_rte_v102_1_sentinel\n - super_glue_wic_v102\n - super_glue_wic_v102_1_sentinel\n - super_glue_wsc_v102_simple_1_sentinel_eval\n - super_glue_wsc_v102_simple_1_sentinel_train\n - super_glue_wsc_v102_simple_eval\n - super_glue_wsc_v102_simple_train\n - trivia_qa_v010\n - wikipedia_20190301.en_v003_unsupervised\n - wmt14_ende_v003\n - wmt14_enfr_v003\n - wmt15_enfr_v003\n - wmt16_enro_v003\n - wmt19_ende_v003\n - wmt_t2t_ende_v003",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1282/4092235101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#glue_mnli_v002\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#super_glue_cb_v102\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m dataset = seqio.get_mixture_or_task(\"glue_wic_v002\").get_dataset(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/seqio/dataset_providers.py\u001b[0m in \u001b[0;36mget_mixture_or_task\u001b[0;34m(task_or_mixture_name)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTaskRegistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_or_mixture_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1517\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1518\u001b[0m         \u001b[0;34m\"No Task or Mixture found with name '%s'. Available:\\n - %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m         (task_or_mixture_name, \"\\n - \".join(sorted(tasks))))\n",
      "\u001b[0;31mValueError\u001b[0m: No Task or Mixture found with name 'glue_wic_v002'. Available:\n - c4_noclean_v020_unsupervised\n - c4_realnewslike_v020_unsupervised\n - c4_v020_unsupervised\n - c4_v220_iid_denoising\n - c4_v220_prefix_lm\n - c4_v220_span_corruption\n - c4_webtextlike_v020_unsupervised\n - cnn_dailymail_v002\n - dpr_v001_simple\n - dpr_v001_simple_1_sentinel\n - glue_ax_v002\n - glue_cola_v002\n - glue_mnli_matched_v002\n - glue_mnli_mismatched_v002\n - glue_mnli_v002\n - glue_mrpc_v002\n - glue_qnli_v002\n - glue_qqp_v002\n - glue_rte_v002\n - glue_sst2_v002\n - glue_stsb_v002\n - glue_wnli_v002\n - glue_wnli_v002_simple_eval\n - squad_v010\n - squad_v010_allanswers\n - squad_v010_allanswers_span\n - squad_v010_context_free\n - super_glue_axb_v102\n - super_glue_axb_v102_1_sentinel\n - super_glue_axg_v102\n - super_glue_axg_v102_1_sentinel\n - super_glue_boolq_v102\n - super_glue_boolq_v102_1_sentinel\n - super_glue_cb_v102\n - super_glue_cb_v102_1_sentinel\n - super_glue_copa_v102\n - super_glue_copa_v102_1_sentinel\n - super_glue_multirc_v102\n - super_glue_multirc_v102_1_sentinel\n - super_glue_record_v102\n - super_glue_record_v102_1_sentinel\n - super_glue_rte_v102\n - super_glue_rte_v102_1_sentinel\n - super_glue_wic_v102\n - super_glue_wic_v102_1_sentinel\n - super_glue_wsc_v102_simple_1_sentinel_eval\n - super_glue_wsc_v102_simple_1_sentinel_train\n - super_glue_wsc_v102_simple_eval\n - super_glue_wsc_v102_simple_train\n - trivia_qa_v010\n - wikipedia_20190301.en_v003_unsupervised\n - wmt14_ende_v003\n - wmt14_enfr_v003\n - wmt15_enfr_v003\n - wmt16_enro_v003\n - wmt19_ende_v003\n - wmt_t2t_ende_v003"
     ]
    }
   ],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"glue_mnli_v002\").get_dataset(\n",
    "    sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e0f66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"idx\": [],\n",
    "    \"inputs_pretokenized\": [],\n",
    "    \"targets_pretokenized\": []\n",
    "}\n",
    "for i, ex in enumerate(iterator):\n",
    "    dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "    dictionary[\"inputs_pretokenized\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\") )\n",
    "    dictionary[\"targets_pretokenized\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02355dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f29c8402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"mnli hypothesis: You won't learn anything by serving overseas.  premise: because actually when you when you do uh service overseas you end up learning something usually that's that's really useful plumbing or farming or or something like that so you're really learning a skill\",\n",
       " 'mnli hypothesis: This is a stringed instrument created by the ancient Aztecs. premise: The geiro, a percussion instrument made of a notched dried gourd, was developed by the Taano Indians.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['inputs_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbed8ea8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contradiction', 'contradiction']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['targets_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68901632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 3.93 MiB (download: 3.93 MiB, generated: Unknown size, total: 3.93 MiB) to /root/tensorflow_datasets/super_glue/boolq/1.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421891c4ab914cb7a82c067b567d1116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ac117c1b934e5f91a5b4c9f63bdbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245a39afcc6c42258fa075959a2cd490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32c5308ca4f4ccea0b5fdfd4040553f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59246b90c55403193e87a87348c72e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/9427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0230b43d710f4cff8d3719010890c51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteCOOR2N/super_glue-train.tfrecord*...:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a816b418c1434067889c796cc1339f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961f3746021b438eb39810534a7fa0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteCOOR2N/super_glue-validation.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84e0f6c9cc543a3b56c05d397b8f12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/3245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860d38ab3fc04abea654b2ea10c53188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteCOOR2N/super_glue-test.tfrecord*...:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset super_glue downloaded and prepared to /root/tensorflow_datasets/super_glue/boolq/1.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = seqio.get_mixture_or_task(\"super_glue_boolq_v102\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c45496",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8416601",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "\n",
    "    for i, ex in enumerate(iterator):\n",
    "        if name =='multirc':\n",
    "            dictionary[\"idx\"].append(ex[\"idx/answer\"]) \n",
    "        elif name == 'record':\n",
    "            dictionary[\"idx\"].append(ex[\"idx/query\"])\n",
    "        else: dictionary[\"idx\"].append(ex[\"idx\"]) \n",
    "\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b534522f",
   "metadata": {},
   "source": [
    "## for mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31f02f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6c5c8655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test_matched\n",
      "test_mismatched\n",
      "validation_matched\n",
      "validation_mismatched\n"
     ]
    }
   ],
   "source": [
    "name=\"mnli\"\n",
    "#super_glue_cb_v102\n",
    "for split in ['train','test_matched','test_mismatched','validation_matched','validation_mismatched']:\n",
    "    print(split)\n",
    "    seqio_mnli_dataset[split]=get_date(name,split,'glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59ef13a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "389d7a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation and test dataset by combining matched and mismatched\n",
    "\n",
    "seqio_mnli_dataset['validation']=concatenate_datasets([seqio_mnli_dataset['validation_matched'],seqio_mnli_dataset['validation_mismatched']])\n",
    "seqio_mnli_dataset['test']=concatenate_datasets([seqio_mnli_dataset['test_matched'],seqio_mnli_dataset['test_mismatched']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7b9e8f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19647\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19643\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd663bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset.push_to_hub(\n",
    "        \"stjokerli/TextToText_mnli_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c36d791",
   "metadata": {},
   "source": [
    "## For superGlue\n",
    "\n",
    "### For 'cb','boolq','rte','copa' ,'wic','multirc','record'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "08469491",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd5f2f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb\n",
      "  train\n",
      "  test\n",
      "  validation\n",
      "boolq\n",
      "  train\n",
      "  test\n",
      "  validation\n",
      "rte\n",
      "  train\n",
      "  test\n",
      "  validation\n",
      "copa\n",
      "  train\n",
      "  test\n",
      "  validation\n",
      "wic\n",
      "  train\n",
      "  test\n",
      "  validation\n",
      "multirc\n",
      "  train\n",
      "  test\n",
      "  validation\n",
      "record\n",
      "  train\n",
      "  test\n",
      "  validation\n"
     ]
    }
   ],
   "source": [
    "# task_dict=\n",
    "for name in ['cb','boolq','rte','copa'\n",
    "            ,'wic','multirc','record'\n",
    "            ]:\n",
    "   \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['train','test','validation']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9c908a",
   "metadata": {},
   "source": [
    "## For wcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4064ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name,subfix,split):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "#     if dataset_name=='superglue':\n",
    "#         seqio_name=f\"super_glue_{name}_v102\"\n",
    "#     elif dataset_name=='glue':\n",
    "#         seqio_name=f\"glue_{name}_v002\"\n",
    "#     else :\n",
    "#         raise f\"dataset_name: {dataset_name} not config\"\n",
    "    seqio_name=f\"super_glue_{name}_v102_simple_{subfix}\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "        dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfa17642",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=DatasetDict()\n",
    "temp['train']=get_data('wsc','train','train')\n",
    "temp['validation']=get_data('wsc','eval','validation')\n",
    "temp['test']=get_data('wsc','eval','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16965a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue['wsc']=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99368ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "        dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "#         print(ex[\"targets_pretokenized\"])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c00eecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axg\n",
      "  test\n",
      "axb\n",
      "  test\n"
     ]
    }
   ],
   "source": [
    "for name in ['axg','axb']:\n",
    "    \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['test']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "357c6017",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    cb: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 56\n",
       "        })\n",
       "    })\n",
       "    boolq: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 9427\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3245\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3270\n",
       "        })\n",
       "    })\n",
       "    rte: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "    copa: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 400\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 500\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 100\n",
       "        })\n",
       "    })\n",
       "    wic: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 5428\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1400\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 638\n",
       "        })\n",
       "    })\n",
       "    multirc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 27243\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 9693\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 4848\n",
       "        })\n",
       "    })\n",
       "    record: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 138854\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 10000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 15176\n",
       "        })\n",
       "    })\n",
       "    wsc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 259\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 104\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 146\n",
       "        })\n",
       "    })\n",
       "    axg: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 356\n",
       "        })\n",
       "    })\n",
       "    axb: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1104\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d994ca77",
   "metadata": {},
   "source": [
    "# push to hugging face data hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "489ebb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b7cf492a2fc4c77a558e7d927cc4cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691a8c821ab743e9b16789d2be9825d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "    text_to_text_SuperGlue[task].push_to_hub(\n",
    "        f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b15a7b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
