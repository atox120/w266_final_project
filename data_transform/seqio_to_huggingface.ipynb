{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613325c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqio\n",
    "# !pip install t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e41181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U huggingface_hub\n",
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a169f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqio\n",
    "import functools\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import t5.data\n",
    "from t5.data import postprocessors\n",
    "from t5.data import preprocessors\n",
    "from t5.data.glue_utils import get_glue_metric\n",
    "from t5.data.glue_utils import get_glue_postprocess_fn\n",
    "from t5.data.glue_utils import get_glue_text_preprocessor\n",
    "from t5.data.glue_utils import get_super_glue_metric\n",
    "from t5.evaluation import metrics\n",
    "import tensorflow_datasets as tfds\n",
    "from t5.models import utils as model_utils\n",
    "import gin\n",
    "from absl import logging\n",
    "from datasets import load_dataset,concatenate_datasets,DatasetDict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56e15b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.2+nightly'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5c8a5b63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.2+nightly'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c4325f24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempting to register duplicate provider: c4_v220_span_corruption",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [44]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m DEFAULT_OUTPUT_FEATURES \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: seqio\u001b[38;5;241m.\u001b[39mFeature(\n\u001b[1;32m      7\u001b[0m         vocabulary\u001b[38;5;241m=\u001b[39mt5\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget_default_vocabulary(), add_eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         vocabulary\u001b[38;5;241m=\u001b[39mt5\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mget_default_vocabulary(), add_eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m }\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# ==================================== C4 ======================================\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Final pretraining task used in Raffel et al., 2019.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mTaskRegistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc4_v220_span_corruption\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseqio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTfdsDataSource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfds_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mc4/en:2.2.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunctools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrekey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minputs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCacheDatasetPlaceholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspan_corruption\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocessors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend_eos_after_trim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEFAULT_OUTPUT_FEATURES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_fns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Baseline pretraining task used in Raffel et al., 2019.\u001b[39;00m\n\u001b[1;32m     35\u001b[0m TaskRegistry\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc4_v220_iid_denoising\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m     source\u001b[38;5;241m=\u001b[39mseqio\u001b[38;5;241m.\u001b[39mTfdsDataSource(tfds_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc4/en:2.2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m     output_features\u001b[38;5;241m=\u001b[39mDEFAULT_OUTPUT_FEATURES,\n\u001b[1;32m     50\u001b[0m     metric_fns\u001b[38;5;241m=\u001b[39m[])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/seqio/dataset_providers.py:1176\u001b[0m, in \u001b[0;36mTaskRegistry.add\u001b[0;34m(cls, name, source, output_features, preprocessors, postprocess_fn, metric_fns, **kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1173\u001b[0m     metric_fns: Optional[Sequence[MetricFnCallable]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1174\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Task:\n\u001b[1;32m   1175\u001b[0m   \u001b[38;5;124;03m\"\"\"See `Task` constructor for docstring.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpostprocess_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_fns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/seqio/dataset_providers.py:145\u001b[0m, in \u001b[0;36mDatasetProviderRegistry.add\u001b[0;34m(cls, name, provider_cls, *provider_args, **provider_kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    141\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to register a class of an invalid type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting instance of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    143\u001b[0m       (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_PROVIDER_TYPE, provider_cls))\n\u001b[1;32m    144\u001b[0m provider \u001b[38;5;241m=\u001b[39m provider_cls(\u001b[38;5;241m*\u001b[39mprovider_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprovider_kwargs)\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_provider\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m provider\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/seqio/dataset_providers.py:121\u001b[0m, in \u001b[0;36mDatasetProviderRegistry.add_provider\u001b[0;34m(cls, name, provider)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;03m\"\"\"Adds a data provider instance to the registry.\"\"\"\u001b[39;00m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_REGISTRY:\n\u001b[0;32m--> 121\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to register duplicate provider: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m name)\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(provider, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_PROVIDER_TYPE):\n\u001b[1;32m    123\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    124\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to register a class of an invalid type. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting instance of \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m    126\u001b[0m       (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_PROVIDER_TYPE, \u001b[38;5;28mtype\u001b[39m(provider)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: Attempting to register duplicate provider: c4_v220_span_corruption"
     ]
    }
   ],
   "source": [
    "TaskRegistry = seqio.TaskRegistry\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True,\n",
    "        required=False),\n",
    "    \"targets\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True)\n",
    "}\n",
    "\n",
    "# ==================================== C4 ======================================\n",
    "# Final pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_span_corruption\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.span_corruption,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Baseline pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_iid_denoising\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.iid_denoising,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Prefix language modeling pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_prefix_lm\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.prefix_lm,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Configurable tasks used for comparisons in Raffel et al., 2019.\n",
    "_c4_config_suffixes = [\"\", \".noclean\", \".realnewslike\", \".webtextlike\"]\n",
    "for config_suffix in _c4_config_suffixes:\n",
    "    TaskRegistry.add(\n",
    "        \"c4{name}_v020_unsupervised\".format(name=config_suffix.replace(\".\", \"_\")),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"c4/en{config}:2.2.0\".format(\n",
    "          config=config_suffix)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.rekey, key_map={\n",
    "                  \"inputs\": None,\n",
    "                  \"targets\": \"text\"\n",
    "              }),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          preprocessors.unsupervised,\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        metric_fns=[])\n",
    "\n",
    "\n",
    "# ================================ Wikipedia ===================================\n",
    "TaskRegistry.add(\n",
    "    \"wikipedia_20190301.en_v003_unsupervised\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wikipedia/20190301.en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.unsupervised,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# =================================== GLUE =====================================\n",
    "for b in tfds.text.glue.Glue.builder_configs.values():\n",
    "    TaskRegistry.add(\n",
    "        \"glue_%s_v002\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/%s:2.0.0\" % b.name,\n",
    "        splits=[\"test\"] if b.name == \"ax\" else None),\n",
    "        preprocessors=[\n",
    "        get_glue_text_preprocessor(b),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=get_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "# =============================== CNN DailyMail ================================\n",
    "TaskRegistry.add(\n",
    "    \"cnn_dailymail_v002\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"cnn_dailymail:3.1.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.summarize,\n",
    "            article_key=\"article\",\n",
    "            summary_key=\"highlights\"),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.rouge],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ==================================== WMT =====================================\n",
    "# Format: year, tfds builder config, tfds version\n",
    "b_configs = [\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"16\", tfds.translate.wmt16.Wmt16Translate.builder_configs[\"ro-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"15\", tfds.translate.wmt15.Wmt15Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"19\", tfds.translate.wmt19.Wmt19Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for prefix, b, tfds_version in b_configs:\n",
    "    TaskRegistry.add(\n",
    "        \"wmt%s_%s%s_v003\" % (prefix, b.language_pair[1], b.language_pair[0]),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"wmt%s_translate/%s:%s\" %\n",
    "                                  (prefix, b.name, tfds_version)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.translate,\n",
    "              source_language=b.language_pair[1],\n",
    "              target_language=b.language_pair[0],\n",
    "          ),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=[metrics.bleu],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Special case for t2t ende.\n",
    "b = tfds.translate.wmt_t2t.WmtT2tTranslate.builder_configs[\"de-en\"]\n",
    "TaskRegistry.add(\n",
    "    \"wmt_t2t_ende_v003\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wmt_t2t_translate/de-en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.translate,\n",
    "            source_language=b.language_pair[1],\n",
    "            target_language=b.language_pair[0]),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.bleu],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= SuperGlue ==================================\n",
    "for b in tfds.text.super_glue.SuperGlue.builder_configs.values():\n",
    "  # We use a simplified version of WSC, defined below\n",
    "    if \"wsc\" in b.name:\n",
    "        continue\n",
    "    if b.name == \"axb\":\n",
    "        glue_preprocessors = [\n",
    "            functools.partial(\n",
    "                preprocessors.rekey,\n",
    "                key_map={\n",
    "                    \"premise\": \"sentence1\",\n",
    "                    \"hypothesis\": \"sentence2\",\n",
    "                    \"label\": \"label\",\n",
    "                    \"idx\": \"idx\",\n",
    "                }),\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "        ]\n",
    "    else:\n",
    "        glue_preprocessors = [\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "    ]\n",
    "    TaskRegistry.add(\n",
    "        \"super_glue_%s_v102\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "          tfds_name=\"super_glue/%s:1.0.2\" % b.name,\n",
    "          splits=[\"test\"] if b.name in [\"axb\", \"axg\"] else None),\n",
    "        preprocessors=glue_preprocessors,\n",
    "        metric_fns=get_super_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "    # Create SuperGLUE tasks with 1 sentinel token added.\n",
    "    seqio.experimental.add_task_with_sentinels(\"super_glue_%s_v102\" % b.name,\n",
    "                                             num_sentinels=1)\n",
    "\n",
    "# ======================== Definite Pronoun Resolution =========================\n",
    "TaskRegistry.add(\n",
    "    \"dpr_v001_simple\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"definite_pronoun_resolution:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.definite_pronoun_resolution_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"dpr_v001_simple\", num_sentinels=1)\n",
    "\n",
    "# =================================== WSC ======================================\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_train\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"train\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.wsc_simple, correct_referent_only=True),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_train\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.wsc_simple, correct_referent_only=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_eval\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "# =================================== WNLI =====================================\n",
    "TaskRegistry.add(\n",
    "    \"glue_wnli_v002_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/wnli:1.0.0\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        preprocessors.wnli_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# =================================== Squad ====================================\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_context_free\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.squad, include_context=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Squad span prediction task instead of text.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers_span\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad_span_space_tokenized,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.span_qa,\n",
    "    metric_fns=[metrics.span_squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Deprecated: Use `squad_v010_allanswers` instead.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= TriviaQA ===================================\n",
    "TaskRegistry.add(\n",
    "    \"trivia_qa_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"trivia_qa/rc:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.trivia_qa,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.trivia_qa_truncate_inputs,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# =============== PrefixLM objectives (not used in the T5 paper) ===============\n",
    "\n",
    "\n",
    "# # Vocabulary (shared by encoder and decoder)\n",
    "# sentencepiece_model_file = \"gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model\"\n",
    "\n",
    "# vocab = seqio.SentencePieceVocabulary(sentencepiece_model_file)\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_encoder_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_encoder_decoder,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"encoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_decoder_only,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_causal_attention\": seqio.Feature(\n",
    "#             vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"c4_v220_full_lm\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.full_lm,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, add_eos=True)\n",
    "#     },\n",
    "#     metric_fns=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4eab036",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f497d778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-04 10:01:56.876266: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n",
      "2022-04-04 10:01:57.573999: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-04 10:01:57.574615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-04 10:01:57.574846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-04 10:01:57.575938: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-04 10:01:57.580592: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-04 10:01:57.580846: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-04 10:01:57.581046: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-04 10:01:59.646581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-04 10:01:59.647016: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-04 10:01:59.647041: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-04-04 10:01:59.647402: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-04 10:01:59.647942: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21137 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"super_glue_axb_v102\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"test\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce70a93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['idx', 'inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets'])\n",
      "{'idx': 134, 'inputs_pretokenized': b'rte hypothesis: Knowledge bases are populated with facts from unstructured text corpora by relation extraction systems. premise: Relation extraction systems populate knowledge bases with facts from unstructured text corpora.', 'inputs': array([    3,    52,    17,    15, 22455,    10, 16113, 14701,    33,\n",
      "           3, 23606,    28,  6688,    45,    73, 16180,    26,  1499,\n",
      "       11736,   127,     9,    57,  4689, 16629,  1002,     5,     3,\n",
      "       17398,    10, 28898, 16629,  1002,  8364,   342,  1103, 14701,\n",
      "          28,  6688,    45,    73, 16180,    26,  1499, 11736,   127,\n",
      "           9,     5,     1], dtype=int32), 'targets_pretokenized': b'entailment', 'targets': array([   3,   35, 5756,  297,    1], dtype=int32)}\n",
      "dict_keys(['idx', 'inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets'])\n",
      "{'idx': 639, 'inputs_pretokenized': b'rte hypothesis: Hummingbirds are really attracted to bright orange and red (hence why the feeders are usually these colours). premise: Hummingbirds are red (hence why the feeders are usually these colours).', 'inputs': array([    3,    52,    17,    15, 22455,    10,   454, 23434,  9414,\n",
      "           7,    33,   310,    44, 11674,    12,  2756,  5470,    11,\n",
      "        1131,    41,   107,  1433,   572,     8, 22916,     7,    33,\n",
      "        1086,   175,  6548,   137,     3, 17398,    10,   454, 23434,\n",
      "        9414,     7,    33,  1131,    41,   107,  1433,   572,     8,\n",
      "       22916,     7,    33,  1086,   175,  6548,   137,     1],\n",
      "      dtype=int32), 'targets_pretokenized': b'not_entailment', 'targets': array([  59,  834,   35, 5756,  297,    1], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "cnt=2\n",
    "for i, ex in enumerate(iterator):\n",
    "    print(ex.keys())\n",
    "    print(ex)\n",
    "    cnt-=1\n",
    "    if cnt==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0c3c90",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91267697",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    elif dataset_name=='squad':\n",
    "        seqio_name= 'squad_v010_allanswers'\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    print(f'getting dataset == {seqio_name}')\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": [],\n",
    "    }\n",
    "    \n",
    "    if name == 'record':\n",
    "        dictionary['answers']=[]\n",
    "\n",
    "    for i, ex in enumerate(iterator):\n",
    "        if name =='multirc':\n",
    "            dictionary[\"idx\"].append(f\"{ex['idx/paragraph']}-{ex['idx/question']}-{ex['idx/answer']}\") \n",
    "        elif name == 'record':\n",
    "            dictionary[\"idx\"].append(ex[\"idx/query\"])\n",
    "            dictionary['answers'].append([i.decode(\"utf-8\") for i in ex[\"answers\"]])\n",
    "        elif name == 'squad':\n",
    "            dictionary[\"idx\"].append(ex[\"id\"].decode(\"utf-8\"))\n",
    "            start_position=0\n",
    "        else: dictionary[\"idx\"].append(ex[\"idx\"]) \n",
    "\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37126c0c",
   "metadata": {},
   "source": [
    "## for mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3169d690",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8ec78eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test_matched\n",
      "test_mismatched\n",
      "validation_matched\n",
      "validation_mismatched\n"
     ]
    }
   ],
   "source": [
    "name=\"mnli\"\n",
    "#super_glue_cb_v102\n",
    "for split in ['train','test_matched','test_mismatched','validation_matched','validation_mismatched']:\n",
    "    print(split)\n",
    "    seqio_mnli_dataset[split]=get_date(name,split,'glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24488b35",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seqio_mnli_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mseqio_mnli_dataset\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seqio_mnli_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ac5c78a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation and test dataset by combining matched and mismatched\n",
    "\n",
    "seqio_mnli_dataset['validation']=concatenate_datasets([seqio_mnli_dataset['validation_matched'],seqio_mnli_dataset['validation_mismatched']])\n",
    "seqio_mnli_dataset['test']=concatenate_datasets([seqio_mnli_dataset['test_matched'],seqio_mnli_dataset['test_mismatched']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8e62142b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19647\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19643\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0d697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset.push_to_hub(\n",
    "        \"stjokerli/TextToText_mnli_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9c8fda",
   "metadata": {},
   "source": [
    "## For superGlue\n",
    "\n",
    "### For 'cb','boolq','rte','copa' ,'wic','multirc','record'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb263bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7688a175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rte\n",
      "  train\n",
      "getting dataset == super_glue_rte_v102\n",
      "  test\n",
      "getting dataset == super_glue_rte_v102\n",
      "  validation\n",
      "getting dataset == super_glue_rte_v102\n"
     ]
    }
   ],
   "source": [
    "# task_dict=\n",
    "for name in [\n",
    "#     'cb','boolq',\n",
    "    'rte',\n",
    "#     'copa'\n",
    "#             ,'wic',\n",
    "#     'multirc',\n",
    "#     'record'\n",
    "#     'axb','axg'\n",
    "            ]:\n",
    "   \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in [\n",
    "        'train',\n",
    "                  'test',\n",
    "                  'validation'\n",
    "    ]:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a84ff0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 277\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb4a0fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_to_text_SuperGlue['axb']['train']=temp['train']\n",
    "# text_to_text_SuperGlue['axg']['train']=temp['train']\n",
    "text_to_text_SuperGlue['axb']['validation']=temp['validation']\n",
    "text_to_text_SuperGlue['axg']['validation']=temp['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "53e12512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    axb: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1104\n",
       "        })\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "    axg: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 356\n",
       "        })\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "    rte: DatasetDict({\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8924b60b",
   "metadata": {},
   "source": [
    "## For wcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eba12df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name,subfix,split):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "#     if dataset_name=='superglue':\n",
    "#         seqio_name=f\"super_glue_{name}_v102\"\n",
    "#     elif dataset_name=='glue':\n",
    "#         seqio_name=f\"glue_{name}_v002\"\n",
    "#     else :\n",
    "#         raise f\"dataset_name: {dataset_name} not config\"\n",
    "    seqio_name=f\"super_glue_{name}_v102_simple_{subfix}\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "        dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0a4af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=DatasetDict()\n",
    "temp['train']=get_data('wsc','train','train')\n",
    "temp['validation']=get_data('wsc','eval','validation')\n",
    "temp['test']=get_data('wsc','eval','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cd04562",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue['wsc']=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3bc15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "        dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "#         print(ex[\"targets_pretokenized\"])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "588306ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axg\n",
      "  test\n",
      "axb\n",
      "  test\n"
     ]
    }
   ],
   "source": [
    "for name in ['axg','axb']:\n",
    "    \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['test']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4804ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    cb: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 56\n",
       "        })\n",
       "    })\n",
       "    boolq: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 9427\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3245\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3270\n",
       "        })\n",
       "    })\n",
       "    rte: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "    copa: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 400\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 500\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 100\n",
       "        })\n",
       "    })\n",
       "    wic: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 5428\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1400\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 638\n",
       "        })\n",
       "    })\n",
       "    multirc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 27243\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 9693\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 4848\n",
       "        })\n",
       "    })\n",
       "    record: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 138854\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 10000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 15176\n",
       "        })\n",
       "    })\n",
       "    wsc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 259\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 104\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 146\n",
       "        })\n",
       "    })\n",
       "    axg: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 356\n",
       "        })\n",
       "    })\n",
       "    axb: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1104\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dedaad",
   "metadata": {},
   "source": [
    "# push to hugging face data hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e1a433a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90362c89c5d343fc83c862c1921be56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e262ddf7dc8b472181efe6e359f63326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda9a4993af4191a34e56b93df71ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "    text_to_text_SuperGlue[task].push_to_hub(\n",
    "        f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cb9da863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    axb: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1104\n",
       "        })\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "    axg: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 356\n",
       "        })\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "    rte: DatasetDict({\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09868b16",
   "metadata": {},
   "source": [
    "# add squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea788201",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e2e98b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad\n",
      "  train\n",
      "getting dataset == squad_v010_allanswers\n",
      "  validation\n",
      "getting dataset == squad_v010_allanswers\n"
     ]
    }
   ],
   "source": [
    "# task_dict=\n",
    "for name in [\n",
    "#     'cb','boolq','rte','copa'\n",
    "#             ,'wic',\n",
    "#     'multirc',\n",
    "#     'record'\n",
    "    'squad'\n",
    "            ]:\n",
    "   \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['train','validation']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"squad\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2156e5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    rte: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "124a88b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 215,\n",
       " 'inputs': 'hypothesis: The advisee wanted advice. premise: The advisor met with the advisee because she wanted to get advice about job applications.',\n",
       " 'targets': 'entailment'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue['axg']['test'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cadb9dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37adc4444c1148f498cd73418586be6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aa33b4897bd426d937ddcdffb76d8ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed702c144574a3c86b85315b066ad31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "#     if task!='rte':\n",
    "        text_to_text_SuperGlue[task].push_to_hub(\n",
    "            f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a23936",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
