{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e56b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqio\n",
    "# !pip install t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76cc9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U huggingface_hub\n",
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edf87ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqio\n",
    "import functools\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import t5.data\n",
    "from t5.data import postprocessors\n",
    "from t5.data import preprocessors\n",
    "from t5.data.glue_utils import get_glue_metric\n",
    "from t5.data.glue_utils import get_glue_postprocess_fn\n",
    "from t5.data.glue_utils import get_glue_text_preprocessor\n",
    "from t5.data.glue_utils import get_super_glue_metric\n",
    "from t5.evaluation import metrics\n",
    "import tensorflow_datasets as tfds\n",
    "from t5.models import utils as model_utils\n",
    "import gin\n",
    "from absl import logging\n",
    "from datasets import load_dataset,concatenate_datasets,DatasetDict\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c833a09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.2+nightly'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73f5a7c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.2+nightly'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed0dc2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seqio.dataset_providers.Task at 0x7fccc6a2a730>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskRegistry = seqio.TaskRegistry\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True,\n",
    "        required=False),\n",
    "    \"targets\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True)\n",
    "}\n",
    "\n",
    "# ==================================== C4 ======================================\n",
    "# Final pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_span_corruption\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.span_corruption,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Baseline pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_iid_denoising\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.iid_denoising,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Prefix language modeling pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_prefix_lm\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.prefix_lm,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Configurable tasks used for comparisons in Raffel et al., 2019.\n",
    "_c4_config_suffixes = [\"\", \".noclean\", \".realnewslike\", \".webtextlike\"]\n",
    "for config_suffix in _c4_config_suffixes:\n",
    "    TaskRegistry.add(\n",
    "        \"c4{name}_v020_unsupervised\".format(name=config_suffix.replace(\".\", \"_\")),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"c4/en{config}:2.2.0\".format(\n",
    "          config=config_suffix)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.rekey, key_map={\n",
    "                  \"inputs\": None,\n",
    "                  \"targets\": \"text\"\n",
    "              }),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          preprocessors.unsupervised,\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        metric_fns=[])\n",
    "\n",
    "\n",
    "# ================================ Wikipedia ===================================\n",
    "TaskRegistry.add(\n",
    "    \"wikipedia_20190301.en_v003_unsupervised\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wikipedia/20190301.en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.unsupervised,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# =================================== GLUE =====================================\n",
    "for b in tfds.text.glue.Glue.builder_configs.values():\n",
    "    TaskRegistry.add(\n",
    "        \"glue_%s_v002\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/%s:2.0.0\" % b.name,\n",
    "        splits=[\"test\"] if b.name == \"ax\" else None),\n",
    "        preprocessors=[\n",
    "        get_glue_text_preprocessor(b),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=get_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "# =============================== CNN DailyMail ================================\n",
    "TaskRegistry.add(\n",
    "    \"cnn_dailymail_v002\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"cnn_dailymail:3.1.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.summarize,\n",
    "            article_key=\"article\",\n",
    "            summary_key=\"highlights\"),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.rouge],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ==================================== WMT =====================================\n",
    "# Format: year, tfds builder config, tfds version\n",
    "b_configs = [\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"16\", tfds.translate.wmt16.Wmt16Translate.builder_configs[\"ro-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"15\", tfds.translate.wmt15.Wmt15Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"19\", tfds.translate.wmt19.Wmt19Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for prefix, b, tfds_version in b_configs:\n",
    "    TaskRegistry.add(\n",
    "        \"wmt%s_%s%s_v003\" % (prefix, b.language_pair[1], b.language_pair[0]),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"wmt%s_translate/%s:%s\" %\n",
    "                                  (prefix, b.name, tfds_version)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.translate,\n",
    "              source_language=b.language_pair[1],\n",
    "              target_language=b.language_pair[0],\n",
    "          ),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=[metrics.bleu],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Special case for t2t ende.\n",
    "b = tfds.translate.wmt_t2t.WmtT2tTranslate.builder_configs[\"de-en\"]\n",
    "TaskRegistry.add(\n",
    "    \"wmt_t2t_ende_v003\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wmt_t2t_translate/de-en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.translate,\n",
    "            source_language=b.language_pair[1],\n",
    "            target_language=b.language_pair[0]),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.bleu],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= SuperGlue ==================================\n",
    "for b in tfds.text.super_glue.SuperGlue.builder_configs.values():\n",
    "  # We use a simplified version of WSC, defined below\n",
    "    if \"wsc\" in b.name:\n",
    "        continue\n",
    "    if b.name == \"axb\":\n",
    "        glue_preprocessors = [\n",
    "            functools.partial(\n",
    "                preprocessors.rekey,\n",
    "                key_map={\n",
    "                    \"premise\": \"sentence1\",\n",
    "                    \"hypothesis\": \"sentence2\",\n",
    "                    \"label\": \"label\",\n",
    "                    \"idx\": \"idx\",\n",
    "                }),\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "        ]\n",
    "    else:\n",
    "        glue_preprocessors = [\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "    ]\n",
    "    TaskRegistry.add(\n",
    "        \"super_glue_%s_v102\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "          tfds_name=\"super_glue/%s:1.0.2\" % b.name,\n",
    "          splits=[\"test\"] if b.name in [\"axb\", \"axg\"] else None),\n",
    "        preprocessors=glue_preprocessors,\n",
    "        metric_fns=get_super_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "    # Create SuperGLUE tasks with 1 sentinel token added.\n",
    "    seqio.experimental.add_task_with_sentinels(\"super_glue_%s_v102\" % b.name,\n",
    "                                             num_sentinels=1)\n",
    "\n",
    "# ======================== Definite Pronoun Resolution =========================\n",
    "TaskRegistry.add(\n",
    "    \"dpr_v001_simple\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"definite_pronoun_resolution:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.definite_pronoun_resolution_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"dpr_v001_simple\", num_sentinels=1)\n",
    "\n",
    "# =================================== WSC ======================================\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_train\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"train\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.wsc_simple, correct_referent_only=True),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_train\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.wsc_simple, correct_referent_only=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_eval\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "# =================================== WNLI =====================================\n",
    "TaskRegistry.add(\n",
    "    \"glue_wnli_v002_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/wnli:1.0.0\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        preprocessors.wnli_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# =================================== Squad ====================================\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_context_free\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.squad, include_context=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Squad span prediction task instead of text.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers_span\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad_span_space_tokenized,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.span_qa,\n",
    "    metric_fns=[metrics.span_squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Deprecated: Use `squad_v010_allanswers` instead.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= TriviaQA ===================================\n",
    "TaskRegistry.add(\n",
    "    \"trivia_qa_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"trivia_qa/rc:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.trivia_qa,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.trivia_qa_truncate_inputs,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# =============== PrefixLM objectives (not used in the T5 paper) ===============\n",
    "\n",
    "\n",
    "# # Vocabulary (shared by encoder and decoder)\n",
    "# sentencepiece_model_file = \"gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model\"\n",
    "\n",
    "# vocab = seqio.SentencePieceVocabulary(sentencepiece_model_file)\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_encoder_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_encoder_decoder,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"encoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_decoder_only,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_causal_attention\": seqio.Feature(\n",
    "#             vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"c4_v220_full_lm\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.full_lm,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, add_eos=True)\n",
    "#     },\n",
    "#     metric_fns=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c42160b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cc6d6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 22:20:42.824645: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"squad_v010_allanswers\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8260fc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'id', 'context', 'question', 'answers'])\n",
      "{'inputs_pretokenized': b\"question: What must be well understood when applying a servo motor for use ? context: A servomotor is a motor , very often sold as a complete module , which is used within a position - control or speed - control feedback control system mainly control valves , such as motor - operated control valves . Servomotors are used in applications such as machine tools , pen plotters , and other process systems . Motors intended for use in a servomechanism must have well - documented characteristics for speed , torque , and power . The speed vs . torque curve is quite important and is high ratio for a servo motor . Dynamic response characteristics such as winding inductance and rotor inertia are also important ; these factors limit the overall performance of the servomechanism loop . Large , powerful , but slow - responding servo loops may use conventional AC or DC motors and drive systems with position or speed feedback on the motor . As dynamic response requirements increase , more specialized motor designs such as coreless motors are used . AC motors ' superior power density and acceleration characteristics compared to that of DC motors tends to favor PM synchronous , BLDC , induction , and SRM drive applications . \", 'inputs': array([  822,    10,   363,   398,    36,   168,  7571,   116,  6247,\n",
      "           3,     9,     3,  3473,    32,  2340,    21,   169,     3,\n",
      "          58,  2625,    10,    71,     3,  3473,    32, 18271,    19,\n",
      "           3,     9,  2340,     3,     6,   182,   557,  1916,    38,\n",
      "           3,     9,   743,  6008,     3,     6,    84,    19,   261,\n",
      "         441,     3,     9,  1102,     3,    18,   610,    42,  1634,\n",
      "           3,    18,   610,  3160,   610,   358,     3,  4894,   610,\n",
      "        9522,     7,     3,     6,   224,    38,  2340,     3,    18,\n",
      "        7747,   610,  9522,     7,     3,     5,     3, 17598,    32,\n",
      "       18271,     7,    33,   261,    16,  1564,   224,    38,  1437,\n",
      "        1339,     3,     6,  4550,  5944,  4849,     3,     6,    11,\n",
      "         119,   433,  1002,     3,     5,  5083,     7,  3855,    21,\n",
      "         169,    16,     3,     9,     3,  3473,  7159,  3441, 14378,\n",
      "         398,    43,   168,     3,    18, 15552,  6803,    21,  1634,\n",
      "           3,     6, 19527,     3,     6,    11,   579,     3,     5,\n",
      "          37,  1634,     3,   208,     7,     3,     5, 19527,  8435,\n",
      "          19,   882,   359,    11,    19,   306,  5688,    21,     3,\n",
      "           9,     3,  3473,    32,  2340,     3,     5, 13967,  3113,\n",
      "        1773,  6803,   224,    38,  2943,    53,    16,  7472,   663,\n",
      "          11,     3,  2719,   127,    16,    49,    17,    23,     9,\n",
      "          33,    92,   359,     3,   117,   175,  2580,  2006,     8,\n",
      "        1879,   821,    13,     8,     3,  3473,  7159,  3441, 14378,\n",
      "        6494,     3,     5,  7199,     3,     6,  2021,     3,     6,\n",
      "          68,  2684,     3,    18, 16523,     3,  3473,    32,  6494,\n",
      "           7,   164,   169,  7450,  5686,    42,  5795,  2340,     7,\n",
      "          11,  1262,  1002,    28,  1102,    42,  1634,  3160,    30,\n",
      "           8,  2340,     3,     5,   282,  4896,  1773,  1502,   993,\n",
      "           3,     6,    72,     3,  8689,  2340,  2888,   224,    38,\n",
      "        2583,   924,  2340,     1], dtype=int32), 'targets_pretokenized': b'speed , torque , and power', 'targets': array([ 1634,     3,     6, 19527,     3,     6,    11,   579,     1],\n",
      "      dtype=int32), 'id': b'5726c204708984140094d0ac', 'context': b\"A servomotor is a motor , very often sold as a complete module , which is used within a position - control or speed - control feedback control system mainly control valves , such as motor - operated control valves . Servomotors are used in applications such as machine tools , pen plotters , and other process systems . Motors intended for use in a servomechanism must have well - documented characteristics for speed , torque , and power . The speed vs . torque curve is quite important and is high ratio for a servo motor . Dynamic response characteristics such as winding inductance and rotor inertia are also important ; these factors limit the overall performance of the servomechanism loop . Large , powerful , but slow - responding servo loops may use conventional AC or DC motors and drive systems with position or speed feedback on the motor . As dynamic response requirements increase , more specialized motor designs such as coreless motors are used . AC motors ' superior power density and acceleration characteristics compared to that of DC motors tends to favor PM synchronous , BLDC , induction , and SRM drive applications . \", 'question': b'What must be well understood when applying a servo motor for use ? ', 'answers': array([b'speed , torque , and power'], dtype=object)}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'id', 'context', 'question', 'answers'])\n",
      "{'inputs_pretokenized': b'question: How is the saxophone features in Ravel \\' s Bolero ? context: Saxophones appear in some scores from the late 19th century onwards . While appearing only as featured solo instruments in some works , for example Maurice Ravel \\' s orchestration of Modest Mussorgsky \\' s Pictures at an Exhibition and Sergei Rachmaninoff \\' s Symphonic Dances , the saxophone is included in other works , such as Ravel \\' s Bol \\xc3\\xa9 ro , Sergei Prokofiev \\' s Romeo and Juliet Suites 1 and 2 and many other works as a member of the orchestral ensemble . The euphonium is featured in a few late Romantic and 20th - century works , usually playing parts marked \" tenor tuba \" , including Gustav Holst \\' s The Planets , and Richard Strauss \\' s Ein Heldenleben . ', 'inputs': array([  822,    10,   571,    19,     8,     3,     7,     9,   226,\n",
      "          32,  6399,   753,    16,  2922,  4911,     3,    31,     3,\n",
      "           7,  8166,    49,    32,     3,    58,  2625,    10,  1138,\n",
      "         226,    32,  6399,     7,  2385,    16,   128,  7586,    45,\n",
      "           8,  1480,   957,   189,  2646,    30,  2239,     7,     3,\n",
      "           5,   818, 16069,   163,    38,  4510,  6729,  7778,    16,\n",
      "         128,   930,     3,     6,    21,   677,  7758,  4920,  2922,\n",
      "        4911,     3,    31,     3,     7, 13873,  1575,    13,  5073,\n",
      "         222,  6887,     7,  1677,  5352,     3,    31,     3,     7,\n",
      "       16571,    44,    46, 22371,    11, 20843,    23,   391,  1836,\n",
      "         348,    77,  1647,     3,    31,     3,     7,  5224,  7656,\n",
      "        4554,  9438,     7,     3,     6,     8,     3,     7,     9,\n",
      "         226,    32,  6399,    19,  1285,    16,   119,   930,     3,\n",
      "           6,   224,    38,  2922,  4911,     3,    31,     3,     7,\n",
      "        8166,     3,   154,     3,    52,    32,     3,     6, 20843,\n",
      "          23,   749,   157,    32,  8549,   208,     3,    31,     3,\n",
      "           7,  7332,    32,    11, 12983,    17,  9254,     7,   209,\n",
      "          11,   204,    11,   186,   119,   930,    38,     3,     9,\n",
      "        1144,    13,     8, 13873,    40,  8784,     3,     5,    37,\n",
      "           3,    15,    76,  9621,  2552,    19,  4510,    16,     3,\n",
      "           9,   360,  1480,  3385,  1225,    11,   460,   189,     3,\n",
      "          18,  2646,   930,     3,     6,  1086,  1556,  1467,  7027,\n",
      "          96,     3,   324,   127,  6244,     9,    96,     3,     6,\n",
      "         379, 27969,  5838,     7,    17,     3,    31,     3,     7,\n",
      "          37, 12601,     7,     3,     6,    11,  4117,  5438,   302,\n",
      "           7,     3,    31,     3,     7,   890, 26099,    35,  8766,\n",
      "           3,     5,     1], dtype=int32), 'targets_pretokenized': b'as a member of the orchestral ensemble', 'targets': array([   38,     3,     9,  1144,    13,     8, 13873,    40,  8784,\n",
      "           1], dtype=int32), 'id': b'56f7241a3d8e2e1400e37392', 'context': b'Saxophones appear in some scores from the late 19th century onwards . While appearing only as featured solo instruments in some works , for example Maurice Ravel \\' s orchestration of Modest Mussorgsky \\' s Pictures at an Exhibition and Sergei Rachmaninoff \\' s Symphonic Dances , the saxophone is included in other works , such as Ravel \\' s Bol \\xc3\\xa9 ro , Sergei Prokofiev \\' s Romeo and Juliet Suites 1 and 2 and many other works as a member of the orchestral ensemble . The euphonium is featured in a few late Romantic and 20th - century works , usually playing parts marked \" tenor tuba \" , including Gustav Holst \\' s The Planets , and Richard Strauss \\' s Ein Heldenleben . ', 'question': b\"How is the saxophone features in Ravel ' s Bolero ? \", 'answers': array([b'as a member of the orchestral ensemble'], dtype=object)}\n"
     ]
    }
   ],
   "source": [
    "cnt=2\n",
    "for i, ex in enumerate(iterator):\n",
    "    print(ex.keys())\n",
    "    print(ex)\n",
    "    cnt-=1\n",
    "    if cnt==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7fdcbf",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9916ce58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    elif dataset_name=='squad':\n",
    "        seqio_name= 'squad_v010_allanswers'\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    print(f'getting dataset == {seqio_name}')\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": [],\n",
    "    }\n",
    "    \n",
    "    if name == 'record':\n",
    "        dictionary['answers']=[]\n",
    "\n",
    "    for i, ex in enumerate(iterator):\n",
    "        if name =='multirc':\n",
    "            dictionary[\"idx\"].append(f\"{ex['idx/paragraph']}-{ex['idx/question']}-{ex['idx/answer']}\") \n",
    "        elif name == 'record':\n",
    "            dictionary[\"idx\"].append(ex[\"idx/query\"])\n",
    "            dictionary['answers'].append([i.decode(\"utf-8\") for i in ex[\"answers\"]])\n",
    "        elif name == 'squad':\n",
    "            dictionary[\"idx\"].append(ex[\"id\"].decode(\"utf-8\"))\n",
    "            start_position=0\n",
    "        else: dictionary[\"idx\"].append(ex[\"idx\"]) \n",
    "\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1be062",
   "metadata": {},
   "source": [
    "## for mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9848f42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e851300e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test_matched\n",
      "test_mismatched\n",
      "validation_matched\n",
      "validation_mismatched\n"
     ]
    }
   ],
   "source": [
    "name=\"mnli\"\n",
    "#super_glue_cb_v102\n",
    "for split in ['train','test_matched','test_mismatched','validation_matched','validation_mismatched']:\n",
    "    print(split)\n",
    "    seqio_mnli_dataset[split]=get_date(name,split,'glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f334606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e65a326f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation and test dataset by combining matched and mismatched\n",
    "\n",
    "seqio_mnli_dataset['validation']=concatenate_datasets([seqio_mnli_dataset['validation_matched'],seqio_mnli_dataset['validation_mismatched']])\n",
    "seqio_mnli_dataset['test']=concatenate_datasets([seqio_mnli_dataset['test_matched'],seqio_mnli_dataset['test_mismatched']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e3620963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19647\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19643\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef790bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset.push_to_hub(\n",
    "        \"stjokerli/TextToText_mnli_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ca4bf8",
   "metadata": {},
   "source": [
    "## For superGlue\n",
    "\n",
    "### For 'cb','boolq','rte','copa' ,'wic','multirc','record'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69110e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b4afa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record\n",
      "  train\n",
      "  test\n",
      "  validation\n"
     ]
    }
   ],
   "source": [
    "# task_dict=\n",
    "for name in [\n",
    "#     'cb','boolq','rte','copa'\n",
    "#             ,'wic',\n",
    "#     'multirc',\n",
    "    'record'\n",
    "            ]:\n",
    "   \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['train','test','validation']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c84fa",
   "metadata": {},
   "source": [
    "## For wcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d23b184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name,subfix,split):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "#     if dataset_name=='superglue':\n",
    "#         seqio_name=f\"super_glue_{name}_v102\"\n",
    "#     elif dataset_name=='glue':\n",
    "#         seqio_name=f\"glue_{name}_v002\"\n",
    "#     else :\n",
    "#         raise f\"dataset_name: {dataset_name} not config\"\n",
    "    seqio_name=f\"super_glue_{name}_v102_simple_{subfix}\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "        dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df9308f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=DatasetDict()\n",
    "temp['train']=get_data('wsc','train','train')\n",
    "temp['validation']=get_data('wsc','eval','validation')\n",
    "temp['test']=get_data('wsc','eval','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2d0f3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue['wsc']=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91f05a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "        dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "#         print(ex[\"targets_pretokenized\"])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e307cebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axg\n",
      "  test\n",
      "axb\n",
      "  test\n"
     ]
    }
   ],
   "source": [
    "for name in ['axg','axb']:\n",
    "    \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['test']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85c061c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    cb: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 56\n",
       "        })\n",
       "    })\n",
       "    boolq: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 9427\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3245\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3270\n",
       "        })\n",
       "    })\n",
       "    rte: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "    copa: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 400\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 500\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 100\n",
       "        })\n",
       "    })\n",
       "    wic: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 5428\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1400\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 638\n",
       "        })\n",
       "    })\n",
       "    multirc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 27243\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 9693\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 4848\n",
       "        })\n",
       "    })\n",
       "    record: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 138854\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 10000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 15176\n",
       "        })\n",
       "    })\n",
       "    wsc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 259\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 104\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 146\n",
       "        })\n",
       "    })\n",
       "    axg: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 356\n",
       "        })\n",
       "    })\n",
       "    axb: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1104\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41723b4",
   "metadata": {},
   "source": [
    "# push to hugging face data hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb5957bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90362c89c5d343fc83c862c1921be56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e262ddf7dc8b472181efe6e359f63326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda9a4993af4191a34e56b93df71ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "    text_to_text_SuperGlue[task].push_to_hub(\n",
    "        f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c8b696b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    record: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets', 'answers'],\n",
       "            num_rows: 138854\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets', 'answers'],\n",
       "            num_rows: 10000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets', 'answers'],\n",
       "            num_rows: 15176\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0412c7",
   "metadata": {},
   "source": [
    "# add squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a9739d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65418e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad\n",
      "  train\n",
      "getting dataset == squad_v010_allanswers\n",
      "  validation\n",
      "getting dataset == squad_v010_allanswers\n"
     ]
    }
   ],
   "source": [
    "# task_dict=\n",
    "for name in [\n",
    "#     'cb','boolq','rte','copa'\n",
    "#             ,'wic',\n",
    "#     'multirc',\n",
    "#     'record'\n",
    "    'squad'\n",
    "            ]:\n",
    "   \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['train','validation']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"squad\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aad6a0c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    squad: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 87599\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 10570\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e8da446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': '573632816c16ec1900b92992',\n",
       " 'inputs': 'question: What does trophy hunting create in Africa ? context: A scientific study in the journal , Biological Conservation , states that trophy hunting is of \" major importance to conservation in Africa by creating economic incentives for conservation over vast areas , including areas which may be unsuitable for alternative wildlife - based land uses such as photographic ecotourism . \" However , another study states that less than 3 % of a trophy hunters \\' expenditures reach the local level , meaning that the economic incentive and benefit is \" minimal , particularly when we consider the vast areas of land that hunting concessions occupy . \" ',\n",
       " 'targets': 'economic incentives for conservation'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue['squad']['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25c34369",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca68f6e033214959a03135a9b2bdfa43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "747d7266d3c641b6abd9b5826eacee51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "    text_to_text_SuperGlue[task].push_to_hub(\n",
    "        f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62af5e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
