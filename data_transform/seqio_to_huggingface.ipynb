{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45c7736e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqio\n",
    "# !pip install t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d6f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U huggingface_hub\n",
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6043fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqio\n",
    "import functools\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import t5.data\n",
    "from t5.data import postprocessors\n",
    "from t5.data import preprocessors\n",
    "from t5.data.glue_utils import get_glue_metric\n",
    "from t5.data.glue_utils import get_glue_postprocess_fn\n",
    "from t5.data.glue_utils import get_glue_text_preprocessor\n",
    "from t5.data.glue_utils import get_super_glue_metric\n",
    "from t5.evaluation import metrics\n",
    "import tensorflow_datasets as tfds\n",
    "from t5.models import utils as model_utils\n",
    "import gin\n",
    "from absl import logging\n",
    "from datasets import load_dataset,concatenate_datasets,DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea8391b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.2+nightly'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10931b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seqio.dataset_providers.Task at 0x7ff66f483190>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskRegistry = seqio.TaskRegistry\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True,\n",
    "        required=False),\n",
    "    \"targets\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True)\n",
    "}\n",
    "\n",
    "# ==================================== C4 ======================================\n",
    "# Final pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_span_corruption\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.span_corruption,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Baseline pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_iid_denoising\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.iid_denoising,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Prefix language modeling pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_prefix_lm\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.prefix_lm,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Configurable tasks used for comparisons in Raffel et al., 2019.\n",
    "_c4_config_suffixes = [\"\", \".noclean\", \".realnewslike\", \".webtextlike\"]\n",
    "for config_suffix in _c4_config_suffixes:\n",
    "    TaskRegistry.add(\n",
    "        \"c4{name}_v020_unsupervised\".format(name=config_suffix.replace(\".\", \"_\")),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"c4/en{config}:2.2.0\".format(\n",
    "          config=config_suffix)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.rekey, key_map={\n",
    "                  \"inputs\": None,\n",
    "                  \"targets\": \"text\"\n",
    "              }),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          preprocessors.unsupervised,\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        metric_fns=[])\n",
    "\n",
    "\n",
    "# ================================ Wikipedia ===================================\n",
    "TaskRegistry.add(\n",
    "    \"wikipedia_20190301.en_v003_unsupervised\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wikipedia/20190301.en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.unsupervised,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# =================================== GLUE =====================================\n",
    "for b in tfds.text.glue.Glue.builder_configs.values():\n",
    "    TaskRegistry.add(\n",
    "        \"glue_%s_v002\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/%s:2.0.0\" % b.name,\n",
    "        splits=[\"test\"] if b.name == \"ax\" else None),\n",
    "        preprocessors=[\n",
    "        get_glue_text_preprocessor(b),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=get_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "# =============================== CNN DailyMail ================================\n",
    "TaskRegistry.add(\n",
    "    \"cnn_dailymail_v002\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"cnn_dailymail:3.1.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.summarize,\n",
    "            article_key=\"article\",\n",
    "            summary_key=\"highlights\"),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.rouge],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ==================================== WMT =====================================\n",
    "# Format: year, tfds builder config, tfds version\n",
    "b_configs = [\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"16\", tfds.translate.wmt16.Wmt16Translate.builder_configs[\"ro-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"15\", tfds.translate.wmt15.Wmt15Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"19\", tfds.translate.wmt19.Wmt19Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for prefix, b, tfds_version in b_configs:\n",
    "    TaskRegistry.add(\n",
    "        \"wmt%s_%s%s_v003\" % (prefix, b.language_pair[1], b.language_pair[0]),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"wmt%s_translate/%s:%s\" %\n",
    "                                  (prefix, b.name, tfds_version)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.translate,\n",
    "              source_language=b.language_pair[1],\n",
    "              target_language=b.language_pair[0],\n",
    "          ),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=[metrics.bleu],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Special case for t2t ende.\n",
    "b = tfds.translate.wmt_t2t.WmtT2tTranslate.builder_configs[\"de-en\"]\n",
    "TaskRegistry.add(\n",
    "    \"wmt_t2t_ende_v003\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wmt_t2t_translate/de-en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.translate,\n",
    "            source_language=b.language_pair[1],\n",
    "            target_language=b.language_pair[0]),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.bleu],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= SuperGlue ==================================\n",
    "for b in tfds.text.super_glue.SuperGlue.builder_configs.values():\n",
    "  # We use a simplified version of WSC, defined below\n",
    "    if \"wsc\" in b.name:\n",
    "        continue\n",
    "    if b.name == \"axb\":\n",
    "        glue_preprocessors = [\n",
    "            functools.partial(\n",
    "                preprocessors.rekey,\n",
    "                key_map={\n",
    "                    \"premise\": \"sentence1\",\n",
    "                    \"hypothesis\": \"sentence2\",\n",
    "                    \"label\": \"label\",\n",
    "                    \"idx\": \"idx\",\n",
    "                }),\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "        ]\n",
    "    else:\n",
    "        glue_preprocessors = [\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "    ]\n",
    "    TaskRegistry.add(\n",
    "        \"super_glue_%s_v102\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "          tfds_name=\"super_glue/%s:1.0.2\" % b.name,\n",
    "          splits=[\"test\"] if b.name in [\"axb\", \"axg\"] else None),\n",
    "        preprocessors=glue_preprocessors,\n",
    "        metric_fns=get_super_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "    # Create SuperGLUE tasks with 1 sentinel token added.\n",
    "    seqio.experimental.add_task_with_sentinels(\"super_glue_%s_v102\" % b.name,\n",
    "                                             num_sentinels=1)\n",
    "\n",
    "# ======================== Definite Pronoun Resolution =========================\n",
    "TaskRegistry.add(\n",
    "    \"dpr_v001_simple\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"definite_pronoun_resolution:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.definite_pronoun_resolution_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"dpr_v001_simple\", num_sentinels=1)\n",
    "\n",
    "# =================================== WSC ======================================\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_train\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"train\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.wsc_simple, correct_referent_only=True),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_train\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.wsc_simple, correct_referent_only=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_eval\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "# =================================== WNLI =====================================\n",
    "TaskRegistry.add(\n",
    "    \"glue_wnli_v002_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/wnli:1.0.0\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        preprocessors.wnli_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# =================================== Squad ====================================\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_context_free\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.squad, include_context=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Squad span prediction task instead of text.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers_span\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad_span_space_tokenized,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.span_qa,\n",
    "    metric_fns=[metrics.span_squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Deprecated: Use `squad_v010_allanswers` instead.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= TriviaQA ===================================\n",
    "TaskRegistry.add(\n",
    "    \"trivia_qa_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"trivia_qa/rc:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.trivia_qa,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.trivia_qa_truncate_inputs,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# =============== PrefixLM objectives (not used in the T5 paper) ===============\n",
    "\n",
    "\n",
    "# # Vocabulary (shared by encoder and decoder)\n",
    "# sentencepiece_model_file = \"gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model\"\n",
    "\n",
    "# vocab = seqio.SentencePieceVocabulary(sentencepiece_model_file)\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_encoder_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_encoder_decoder,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"encoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_decoder_only,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_causal_attention\": seqio.Feature(\n",
    "#             vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"c4_v220_full_lm\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.full_lm,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, add_eos=True)\n",
    "#     },\n",
    "#     metric_fns=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4da053bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"super_glue_axg_v102\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"test\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6ae210f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['idx', 'inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets'])\n",
      "{'idx': 104, 'inputs_pretokenized': b\"rte hypothesis: The surgeon's affection had grown over time. premise: The surgeon operated on the child with great care; his affection had grown over time.\", 'inputs': array([    3,    52,    17,    15, 22455,    10,    37, 12177,    31,\n",
      "           7, 18615,   141,  4503,   147,    97,     5,     3, 17398,\n",
      "          10,    37, 12177,  7747,    30,     8,   861,    28,   248,\n",
      "         124,   117,   112, 18615,   141,  4503,   147,    97,     5,\n",
      "           1], dtype=int32), 'targets_pretokenized': b'entailment', 'targets': array([   3,   35, 5756,  297,    1], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "for i, ex in enumerate(iterator):\n",
    "    print(ex.keys())\n",
    "    print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30e6da27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: John hired Bill to take care of *him* .', 'inputs': array([    3,   210,     7,    75,    10,  1079, 10626,  3259,    12,\n",
      "         240,   124,    13,  1429, 10813,  1935,     3,     5,     1],\n",
      "      dtype=int32), 'targets_pretokenized': b'John', 'targets': array([1079,    1], dtype=int32), 'label': 1, 'idx': 461}\n"
     ]
    }
   ],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"super_glue_wsc_axg_n\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "for i, ex in enumerate(iterator):\n",
    "    print(ex.keys())\n",
    "    print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e555fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c714e0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"idx\": [],\n",
    "    \"inputs_pretokenized\": [],\n",
    "    \"targets_pretokenized\": []\n",
    "}\n",
    "for i, ex in enumerate(iterator):\n",
    "    dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "    dictionary[\"inputs_pretokenized\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\") )\n",
    "    dictionary[\"targets_pretokenized\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4220b300",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d238609a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idx', 'inputs_pretokenized', 'targets_pretokenized']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b56ab464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"mnli hypothesis: You won't learn anything by serving overseas.  premise: because actually when you when you do uh service overseas you end up learning something usually that's that's really useful plumbing or farming or or something like that so you're really learning a skill\",\n",
       " 'mnli hypothesis: This is a stringed instrument created by the ancient Aztecs. premise: The geiro, a percussion instrument made of a notched dried gourd, was developed by the Taano Indians.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['inputs_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e114e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contradiction', 'contradiction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['targets_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7962d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"glue_mnli_v002\").get_dataset(\n",
    "    sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c791ef19",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"idx\": [],\n",
    "    \"inputs_pretokenized\": [],\n",
    "    \"targets_pretokenized\": []\n",
    "}\n",
    "for i, ex in enumerate(iterator):\n",
    "    dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "    dictionary[\"inputs_pretokenized\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\") )\n",
    "    dictionary[\"targets_pretokenized\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb1eccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a6e379b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"mnli hypothesis: You won't learn anything by serving overseas.  premise: because actually when you when you do uh service overseas you end up learning something usually that's that's really useful plumbing or farming or or something like that so you're really learning a skill\",\n",
       " 'mnli hypothesis: This is a stringed instrument created by the ancient Aztecs. premise: The geiro, a percussion instrument made of a notched dried gourd, was developed by the Taano Indians.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['inputs_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e160f74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contradiction', 'contradiction']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['targets_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf9616a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 3.93 MiB (download: 3.93 MiB, generated: Unknown size, total: 3.93 MiB) to /root/tensorflow_datasets/super_glue/boolq/1.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421891c4ab914cb7a82c067b567d1116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ac117c1b934e5f91a5b4c9f63bdbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245a39afcc6c42258fa075959a2cd490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32c5308ca4f4ccea0b5fdfd4040553f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59246b90c55403193e87a87348c72e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/9427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0230b43d710f4cff8d3719010890c51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteCOOR2N/super_glue-train.tfrecord*...:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a816b418c1434067889c796cc1339f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961f3746021b438eb39810534a7fa0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteCOOR2N/super_glue-validation.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84e0f6c9cc543a3b56c05d397b8f12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/3245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860d38ab3fc04abea654b2ea10c53188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteCOOR2N/super_glue-test.tfrecord*...:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset super_glue downloaded and prepared to /root/tensorflow_datasets/super_glue/boolq/1.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = seqio.get_mixture_or_task(\"super_glue_boolq_v102\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c5f20d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "840955cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "#         dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a98f3a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3522639a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9e4bde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce9abe84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test_matched\n",
      "test_mismatched\n",
      "validation_matched\n",
      "validation_mismatched\n"
     ]
    }
   ],
   "source": [
    "name=\"mnli\"\n",
    "#super_glue_cb_v102\n",
    "for split in ['train','test_matched','test_mismatched','validation_matched','validation_mismatched']:\n",
    "    print(split)\n",
    "    seqio_mnli_dataset[split]=get_date(name,split,'glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7de8b830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "884f0704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hypothesis: He laughed when he talked about finding the new dog. premise: Thank you, I will. He laughed rather ruefully, as he described how he had discovered a very rare species of fern in an inaccessible place, and in his efforts to obtain it had lost his footing, and slipped ignominiously into a neighbouring pond. '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset['train']['inputs'][392701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d02079f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset['train']['targets'][392701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "118137bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation and test dataset by combining matched and mismatched\n",
    "\n",
    "seqio_mnli_dataset['validation']=concatenate_datasets([seqio_mnli_dataset['validation_matched'],seqio_mnli_dataset['validation_mismatched']])\n",
    "seqio_mnli_dataset['test']=concatenate_datasets([seqio_mnli_dataset['test_matched'],seqio_mnli_dataset['test_mismatched']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e46a9edc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328a58abcf2b42d3a80e3a63ef139277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test_matched to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08b0bd18bb34a9cab748158980d42af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test_mismatched to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "782031a1ede94f1c9bf668ee8d27a984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation_matched to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98062290a7e4490ab32a6cab02a526c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation_mismatched to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6a13c2cc7ea47cda78174a64ad7e132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb6f6d9036943ebbb98497a7ed2c755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ddecd0a8107465ba1ac4d1a413c1f03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seqio_mnli_dataset.push_to_hub(\n",
    "        \"stjokerli/TextToText_mnli_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5eb6dadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration stjokerli--TextToText_mnli_seqio-a59b92b1393c0e0b\n",
      "WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/parquet/stjokerli--TextToText_mnli_seqio-a59b92b1393c0e0b/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26d0def125f4a22bc599b50b2a019c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19643\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19647\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset=load_dataset(\"stjokerli/TextToText_mnli_seqio\")\n",
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e549bc93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hypothesis: He laughed when he talked about finding the new dog. premise: Thank you, I will. He laughed rather ruefully, as he described how he had discovered a very rare species of fern in an inaccessible place, and in his efforts to obtain it had lost his footing, and slipped ignominiously into a neighbouring pond. '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset['train']['inputs'][392701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "656a2a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'contradiction'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset['train']['targets'][392701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "51ce3377",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6c21326a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wsc\n",
      "  train\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No Task or Mixture found with name 'super_glue_wsc_v102'. Available:\n - c4_noclean_v020_unsupervised\n - c4_realnewslike_v020_unsupervised\n - c4_v020_unsupervised\n - c4_v220_iid_denoising\n - c4_v220_prefix_lm\n - c4_v220_span_corruption\n - c4_webtextlike_v020_unsupervised\n - cnn_dailymail_v002\n - dpr_v001_simple\n - dpr_v001_simple_1_sentinel\n - glue_ax_v002\n - glue_cola_v002\n - glue_mnli_matched_v002\n - glue_mnli_mismatched_v002\n - glue_mnli_v002\n - glue_mrpc_v002\n - glue_qnli_v002\n - glue_qqp_v002\n - glue_rte_v002\n - glue_sst2_v002\n - glue_stsb_v002\n - glue_wnli_v002\n - glue_wnli_v002_simple_eval\n - squad_v010\n - squad_v010_allanswers\n - squad_v010_allanswers_span\n - squad_v010_context_free\n - super_glue_axb_v102\n - super_glue_axb_v102_1_sentinel\n - super_glue_axg_v102\n - super_glue_axg_v102_1_sentinel\n - super_glue_boolq_v102\n - super_glue_boolq_v102_1_sentinel\n - super_glue_cb_v102\n - super_glue_cb_v102_1_sentinel\n - super_glue_copa_v102\n - super_glue_copa_v102_1_sentinel\n - super_glue_multirc_v102\n - super_glue_multirc_v102_1_sentinel\n - super_glue_record_v102\n - super_glue_record_v102_1_sentinel\n - super_glue_rte_v102\n - super_glue_rte_v102_1_sentinel\n - super_glue_wic_v102\n - super_glue_wic_v102_1_sentinel\n - super_glue_wsc_v102_simple_1_sentinel_eval\n - super_glue_wsc_v102_simple_1_sentinel_train\n - super_glue_wsc_v102_simple_eval\n - super_glue_wsc_v102_simple_train\n - trivia_qa_v010\n - wikipedia_20190301.en_v003_unsupervised\n - wmt14_ende_v003\n - wmt14_enfr_v003\n - wmt15_enfr_v003\n - wmt16_enro_v003\n - wmt19_ende_v003\n - wmt_t2t_ende_v003",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1019/2397165807.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mtemp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"superglue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtext_to_text_SuperGlue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1019/362058846.py\u001b[0m in \u001b[0;36mget_date\u001b[0;34m(name, split, dataset_name)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0;34mf\"dataset_name: {dataset_name} not config\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/seqio/dataset_providers.py\u001b[0m in \u001b[0;36mget_mixture_or_task\u001b[0;34m(task_or_mixture_name)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTaskRegistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_or_mixture_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1517\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1518\u001b[0m         \u001b[0;34m\"No Task or Mixture found with name '%s'. Available:\\n - %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m         (task_or_mixture_name, \"\\n - \".join(sorted(tasks))))\n",
      "\u001b[0;31mValueError\u001b[0m: No Task or Mixture found with name 'super_glue_wsc_v102'. Available:\n - c4_noclean_v020_unsupervised\n - c4_realnewslike_v020_unsupervised\n - c4_v020_unsupervised\n - c4_v220_iid_denoising\n - c4_v220_prefix_lm\n - c4_v220_span_corruption\n - c4_webtextlike_v020_unsupervised\n - cnn_dailymail_v002\n - dpr_v001_simple\n - dpr_v001_simple_1_sentinel\n - glue_ax_v002\n - glue_cola_v002\n - glue_mnli_matched_v002\n - glue_mnli_mismatched_v002\n - glue_mnli_v002\n - glue_mrpc_v002\n - glue_qnli_v002\n - glue_qqp_v002\n - glue_rte_v002\n - glue_sst2_v002\n - glue_stsb_v002\n - glue_wnli_v002\n - glue_wnli_v002_simple_eval\n - squad_v010\n - squad_v010_allanswers\n - squad_v010_allanswers_span\n - squad_v010_context_free\n - super_glue_axb_v102\n - super_glue_axb_v102_1_sentinel\n - super_glue_axg_v102\n - super_glue_axg_v102_1_sentinel\n - super_glue_boolq_v102\n - super_glue_boolq_v102_1_sentinel\n - super_glue_cb_v102\n - super_glue_cb_v102_1_sentinel\n - super_glue_copa_v102\n - super_glue_copa_v102_1_sentinel\n - super_glue_multirc_v102\n - super_glue_multirc_v102_1_sentinel\n - super_glue_record_v102\n - super_glue_record_v102_1_sentinel\n - super_glue_rte_v102\n - super_glue_rte_v102_1_sentinel\n - super_glue_wic_v102\n - super_glue_wic_v102_1_sentinel\n - super_glue_wsc_v102_simple_1_sentinel_eval\n - super_glue_wsc_v102_simple_1_sentinel_train\n - super_glue_wsc_v102_simple_eval\n - super_glue_wsc_v102_simple_train\n - trivia_qa_v010\n - wikipedia_20190301.en_v003_unsupervised\n - wmt14_ende_v003\n - wmt14_enfr_v003\n - wmt15_enfr_v003\n - wmt16_enro_v003\n - wmt19_ende_v003\n - wmt_t2t_ende_v003"
     ]
    }
   ],
   "source": [
    "# task_dict=\n",
    "for name in ['wsc']:\n",
    "# ['cb','boolq','rte','copa','multirc']\n",
    "# [,'wic','multirc','record']:\n",
    "    \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['train','test','validation']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c8938930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    multirc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 27243\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 9693\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 4848\n",
       "        })\n",
       "    })\n",
       "    record: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 138854\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 10000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 15176\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "51f4f7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfba435fe95343fbb8fe95875b74dbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b41e86720494128b5ae41e1d9ebb016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-13 09:35:59.264286: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acda87aded0c4cbcba66b2bc003beffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef3fd9896d240b2b8dccbae0d7dda9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf5c85afa6f4062ba831191e3b841d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4ac9a9a45634578942e28b3392ffd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "HTTPError",
     "evalue": "502 Server Error: Bad Gateway for url: https://huggingface.co/api/datasets/stjokerli/TextToText_record_seqio/upload/main/data/validation-00000-of-00001.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1019/3606420488.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_to_text_SuperGlue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     text_to_text_SuperGlue[task].push_to_hub(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0;34mf\"stjokerli/TextToText_{task}_seqio\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_id, private, token, branch, shard_size)\u001b[0m\n\u001b[1;32m    949\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Pushing split {split} to the Hub.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0;31m# The split=key needs to be removed before merging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             repo_id, split, uploaded_size, dataset_nbytes = self[split]._push_parquet_shards_to_hub(\n\u001b[0m\u001b[1;32m    952\u001b[0m                 \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbranch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbranch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshard_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshard_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             )\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_push_parquet_shards_to_hub\u001b[0;34m(self, repo_id, split, private, token, branch, shard_size)\u001b[0m\n\u001b[1;32m   3554\u001b[0m             \u001b[0mshard\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3555\u001b[0m             \u001b[0muploaded_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3556\u001b[0;31m             api.upload_file(\n\u001b[0m\u001b[1;32m   3557\u001b[0m                 \u001b[0mpath_or_fileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3558\u001b[0m                 \u001b[0mpath_in_repo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_in_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, identical_ok)\u001b[0m\n\u001b[1;32m   1344\u001b[0m                 )\n\u001b[1;32m   1345\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1346\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, identical_ok)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1337\u001b[0;31m             \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midentical_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 502 Server Error: Bad Gateway for url: https://huggingface.co/api/datasets/stjokerli/TextToText_record_seqio/upload/main/data/validation-00000-of-00001.parquet"
     ]
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "    text_to_text_SuperGlue[task].push_to_hub(\n",
    "        f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3b73b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5031ac3dc774d85b5b45c68479aa8af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/904 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration stjokerli--TextToText_cb_seqio-325b43edbbb44887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 125.94 KiB, generated: 209.85 KiB, post-processed: Unknown size, total: 335.79 KiB) to /root/.cache/huggingface/datasets/parquet/stjokerli--TextToText_cb_seqio-325b43edbbb44887/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db5be7ac19b46e48a1617b197a46c66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57167a2b3f2b480e9204eda4aec7e139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/54.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced08f16afbc4a11b128523696010878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/57.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8ab699e3d6472eacd8c327f7c17c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6363752b2afa4036ade1951ac485ae39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/stjokerli--TextToText_cb_seqio-325b43edbbb44887/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590ec3dc10a348069bb169dc4eff5ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp=load_dataset(\"stjokerli/TextToText_cb_seqio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "855da4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hypothesis: he's been to America premise: And I don't want to have to lie to them. The kidnappers have given us until October the eleventh to deliver the document and I haven't despaired of finding it before then. But if the police learn I 've been to America they 'll ask why.\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['validation']['inputs'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9246a860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'entailment'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['validation']['targets'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c5a323",
   "metadata": {},
   "source": [
    "For wcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7f91bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name,subfix,split):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "#     if dataset_name=='superglue':\n",
    "#         seqio_name=f\"super_glue_{name}_v102\"\n",
    "#     elif dataset_name=='glue':\n",
    "#         seqio_name=f\"glue_{name}_v002\"\n",
    "#     else :\n",
    "#         raise f\"dataset_name: {dataset_name} not config\"\n",
    "    seqio_name=f\"super_glue_{name}_v102_simple_{subfix}\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "#         \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "#         dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0e8d5874",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=DatasetDict()\n",
    "temp['train']=get_data('wsc','train','train')\n",
    "temp['validation']=get_data('wsc','eval','validation')\n",
    "temp['test']=get_data('wsc','eval','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "45688468",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue['wsc']=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eb3a7dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    wsc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 259\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 104\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 146\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6170bd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6609e0f24094b09a4878157c6c17508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d72e48ff83474cadbd3563dee71384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "    text_to_text_SuperGlue[task].push_to_hub(\n",
    "        f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "cbc5f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "#         \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "#         dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "#         print(ex[\"targets_pretokenized\"])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "af13c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axg\n",
      "  test\n",
      "axb\n",
      "  test\n"
     ]
    }
   ],
   "source": [
    "text_to_text_SuperGlue=DatasetDict()\n",
    "# task_dict=\n",
    "for name in ['axg','axb']:\n",
    "    \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['test']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "0a83f06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    axg: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 356\n",
       "        })\n",
       "    })\n",
       "    axb: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['inputs', 'targets'],\n",
       "            num_rows: 1104\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "dc385a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e25c8ba79d455d808342a5735f4a5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c99c50b11b457ca9fb4a29183bd29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "    text_to_text_SuperGlue[task].push_to_hub(\n",
    "        f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f899a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
