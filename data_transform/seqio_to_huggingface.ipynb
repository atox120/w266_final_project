{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d8a890a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seqio\n",
    "# !pip install t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dab91de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U huggingface_hub\n",
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6e0c47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqio\n",
    "import functools\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import t5.data\n",
    "from t5.data import postprocessors\n",
    "from t5.data import preprocessors\n",
    "from t5.data.glue_utils import get_glue_metric\n",
    "from t5.data.glue_utils import get_glue_postprocess_fn\n",
    "from t5.data.glue_utils import get_glue_text_preprocessor\n",
    "from t5.data.glue_utils import get_super_glue_metric\n",
    "from t5.evaluation import metrics\n",
    "import tensorflow_datasets as tfds\n",
    "from t5.models import utils as model_utils\n",
    "import gin\n",
    "from absl import logging\n",
    "from datasets import load_dataset,concatenate_datasets,DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b67343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.2+nightly'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30cfab88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.2+nightly'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40e2c429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seqio.dataset_providers.Task at 0x7f96cdd2c7f0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskRegistry = seqio.TaskRegistry\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True,\n",
    "        required=False),\n",
    "    \"targets\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True)\n",
    "}\n",
    "\n",
    "# ==================================== C4 ======================================\n",
    "# Final pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_span_corruption\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.span_corruption,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Baseline pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_iid_denoising\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.iid_denoising,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Prefix language modeling pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_prefix_lm\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.prefix_lm,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Configurable tasks used for comparisons in Raffel et al., 2019.\n",
    "_c4_config_suffixes = [\"\", \".noclean\", \".realnewslike\", \".webtextlike\"]\n",
    "for config_suffix in _c4_config_suffixes:\n",
    "    TaskRegistry.add(\n",
    "        \"c4{name}_v020_unsupervised\".format(name=config_suffix.replace(\".\", \"_\")),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"c4/en{config}:2.2.0\".format(\n",
    "          config=config_suffix)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.rekey, key_map={\n",
    "                  \"inputs\": None,\n",
    "                  \"targets\": \"text\"\n",
    "              }),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          preprocessors.unsupervised,\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        metric_fns=[])\n",
    "\n",
    "\n",
    "# ================================ Wikipedia ===================================\n",
    "TaskRegistry.add(\n",
    "    \"wikipedia_20190301.en_v003_unsupervised\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wikipedia/20190301.en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.unsupervised,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# =================================== GLUE =====================================\n",
    "for b in tfds.text.glue.Glue.builder_configs.values():\n",
    "    TaskRegistry.add(\n",
    "        \"glue_%s_v002\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/%s:2.0.0\" % b.name,\n",
    "        splits=[\"test\"] if b.name == \"ax\" else None),\n",
    "        preprocessors=[\n",
    "        get_glue_text_preprocessor(b),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=get_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "# =============================== CNN DailyMail ================================\n",
    "TaskRegistry.add(\n",
    "    \"cnn_dailymail_v002\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"cnn_dailymail:3.1.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.summarize,\n",
    "            article_key=\"article\",\n",
    "            summary_key=\"highlights\"),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.rouge],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ==================================== WMT =====================================\n",
    "# Format: year, tfds builder config, tfds version\n",
    "b_configs = [\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"16\", tfds.translate.wmt16.Wmt16Translate.builder_configs[\"ro-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"15\", tfds.translate.wmt15.Wmt15Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"19\", tfds.translate.wmt19.Wmt19Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for prefix, b, tfds_version in b_configs:\n",
    "    TaskRegistry.add(\n",
    "        \"wmt%s_%s%s_v003\" % (prefix, b.language_pair[1], b.language_pair[0]),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"wmt%s_translate/%s:%s\" %\n",
    "                                  (prefix, b.name, tfds_version)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.translate,\n",
    "              source_language=b.language_pair[1],\n",
    "              target_language=b.language_pair[0],\n",
    "          ),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=[metrics.bleu],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Special case for t2t ende.\n",
    "b = tfds.translate.wmt_t2t.WmtT2tTranslate.builder_configs[\"de-en\"]\n",
    "TaskRegistry.add(\n",
    "    \"wmt_t2t_ende_v003\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wmt_t2t_translate/de-en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.translate,\n",
    "            source_language=b.language_pair[1],\n",
    "            target_language=b.language_pair[0]),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.bleu],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= SuperGlue ==================================\n",
    "for b in tfds.text.super_glue.SuperGlue.builder_configs.values():\n",
    "  # We use a simplified version of WSC, defined below\n",
    "    if \"wsc\" in b.name:\n",
    "        continue\n",
    "    if b.name == \"axb\":\n",
    "        glue_preprocessors = [\n",
    "            functools.partial(\n",
    "                preprocessors.rekey,\n",
    "                key_map={\n",
    "                    \"premise\": \"sentence1\",\n",
    "                    \"hypothesis\": \"sentence2\",\n",
    "                    \"label\": \"label\",\n",
    "                    \"idx\": \"idx\",\n",
    "                }),\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "        ]\n",
    "    else:\n",
    "        glue_preprocessors = [\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "    ]\n",
    "    TaskRegistry.add(\n",
    "        \"super_glue_%s_v102\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "          tfds_name=\"super_glue/%s:1.0.2\" % b.name,\n",
    "          splits=[\"test\"] if b.name in [\"axb\", \"axg\"] else None),\n",
    "        preprocessors=glue_preprocessors,\n",
    "        metric_fns=get_super_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "    # Create SuperGLUE tasks with 1 sentinel token added.\n",
    "    seqio.experimental.add_task_with_sentinels(\"super_glue_%s_v102\" % b.name,\n",
    "                                             num_sentinels=1)\n",
    "\n",
    "# ======================== Definite Pronoun Resolution =========================\n",
    "TaskRegistry.add(\n",
    "    \"dpr_v001_simple\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"definite_pronoun_resolution:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.definite_pronoun_resolution_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"dpr_v001_simple\", num_sentinels=1)\n",
    "\n",
    "# =================================== WSC ======================================\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_train\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"train\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.wsc_simple, correct_referent_only=True),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_train\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.wsc_simple, correct_referent_only=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_eval\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "# =================================== WNLI =====================================\n",
    "TaskRegistry.add(\n",
    "    \"glue_wnli_v002_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/wnli:1.0.0\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        preprocessors.wnli_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# =================================== Squad ====================================\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_context_free\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.squad, include_context=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Squad span prediction task instead of text.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers_span\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad_span_space_tokenized,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.span_qa,\n",
    "    metric_fns=[metrics.span_squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Deprecated: Use `squad_v010_allanswers` instead.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= TriviaQA ===================================\n",
    "TaskRegistry.add(\n",
    "    \"trivia_qa_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"trivia_qa/rc:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.trivia_qa,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.trivia_qa_truncate_inputs,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# =============== PrefixLM objectives (not used in the T5 paper) ===============\n",
    "\n",
    "\n",
    "# # Vocabulary (shared by encoder and decoder)\n",
    "# sentencepiece_model_file = \"gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model\"\n",
    "\n",
    "# vocab = seqio.SentencePieceVocabulary(sentencepiece_model_file)\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_encoder_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_encoder_decoder,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"encoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_decoder_only,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_causal_attention\": seqio.Feature(\n",
    "#             vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"c4_v220_full_lm\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.full_lm,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, add_eos=True)\n",
    "#     },\n",
    "#     metric_fns=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8656e26b",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a79d7e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 31.98 KiB (download: 31.98 KiB, generated: Unknown size, total: 31.98 KiB) to ~/tensorflow_datasets/super_glue/wsc.fixed/1.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca4b9391f5b497f99e6cd9b2e57a023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7d578545e394b72a3bacb9fbac68b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54c18056d417447f9daf9b5d164f9f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f6f05d34134ec88bf334be7f992ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16ab704fdeaa42d1b6152d590e3a564b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/554 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d44e323869475c91006d3752d98fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/wsc.fixed/1.0.2.incompleteXAZSZO/super_glue-train.tfrecord*...:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8239b5ff012428b86dfa614d88cadaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/104 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d72e9bd3ef7a4232a00f345042045b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/wsc.fixed/1.0.2.incompleteXAZSZO/super_glue-validation.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "624958e961af4feb96f55aca2aca4c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/146 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd686ddf12c94566af5947353ad60b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/wsc.fixed/1.0.2.incompleteXAZSZO/super_glue-test.tfrecord*...:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset super_glue downloaded and prepared to ~/tensorflow_datasets/super_glue/wsc.fixed/1.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"super_glue_wsc_v102_simple_train\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d8c7f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: The politicians far away in Washington could not know the settlers so *they* must make rules to regulate them.', 'inputs': array([    3,   210,     7,    75,    10,    37, 13446,   623,   550,\n",
      "          16,  2386,   228,    59,   214,     8,     3, 29547,    78,\n",
      "        1429, 11056,  1935,   398,   143,  2219,    12, 16363,   135,\n",
      "           5,     1], dtype=int32), 'targets_pretokenized': b'politicians', 'targets': array([13446,     1], dtype=int32), 'label': 1, 'idx': 175}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: The cat was lying by the mouse hole waiting for the mouse , but *it* was too impatient.', 'inputs': array([    3,   210,     7,    75,    10,    37,  1712,    47, 12267,\n",
      "          57,     8,  8429,  6356,  2794,    21,     8,  8429,     3,\n",
      "           6,    68,  1429,   155,  1935,    47,   396,   256, 10061,\n",
      "           5,     1], dtype=int32), 'targets_pretokenized': b'The cat', 'targets': array([  37, 1712,    1], dtype=int32), 'label': 1, 'idx': 295}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: Babar wonders how he can get new clothing. Luckily, a very rich old man who has always been fond of little elephants understands right away that he is longing for a fine suit. As he likes to make people happy, he gives him *his* wallet.', 'inputs': array([    3,   210,     7,    75,    10,  2659,  1047,  3337,     7,\n",
      "         149,     3,    88,    54,   129,   126,  5698,     5,     3,\n",
      "       16996,     6,     3,     9,   182,  2354,   625,   388,   113,\n",
      "          65,   373,   118,  3036,    13,   385, 17926,     7,   734,\n",
      "           7,   269,   550,    24,     3,    88,    19,   307,    53,\n",
      "          21,     3,     9,  1399,  3237,     5,   282,     3,    88,\n",
      "         114,     7,    12,   143,   151,  1095,     6,     3,    88,\n",
      "        1527,   376,  1429, 10193,  1935, 11725,     5,     1],\n",
      "      dtype=int32), 'targets_pretokenized': b'old man', 'targets': array([625, 388,   1], dtype=int32), 'label': 1, 'idx': 404}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: Anne gave birth to a daughter last month. *She* is a very charming woman.', 'inputs': array([    3,   210,     7,    75,    10,  8977,  1891,  3879,    12,\n",
      "           3,     9,  3062,   336,   847,     5,  1429, 12736,  1935,\n",
      "          19,     3,     9,   182, 12216,  2335,     5,     1],\n",
      "      dtype=int32), 'targets_pretokenized': b'Anne', 'targets': array([8977,    1], dtype=int32), 'label': 1, 'idx': 103}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: After I saw Bill catching flies and pulling off their wings, I boxed his ears. I showed the master the flies, some crushed and some crawling about helpless, and I showed him the wings on the window sill. I never saw him so angry before; but as Bill was still howling and whining, like the coward that he was, he did not give him any more punishment of that kind, but set *him* up on a stool for the rest of the afternoon, and said that he should not go out to play for that week.', 'inputs': array([    3,   210,     7,    75,    10,   621,    27,  1509,  3259,\n",
      "           3, 11907,     3,    89,  4664,    11, 12473,   326,    70,\n",
      "       13943,     6,    27,  1367,    15,    26,   112, 11581,     5,\n",
      "          27,  3217,     8,  2325,     8,     3,    89,  4664,     6,\n",
      "         128, 19858,    11,   128, 18639,    53,    81,   199,   924,\n",
      "           6,    11,    27,  3217,   376,     8, 13943,    30,     8,\n",
      "        2034,   108,   195,     5,    27,   470,  1509,   376,    78,\n",
      "       12603,   274,   117,    68,    38,  3259,    47,   341,   149,\n",
      "         697,    11, 14228,    77,    53,     6,   114,     8,   576,\n",
      "        2239,    24,     3,    88,    47,     6,     3,    88,   410,\n",
      "          59,   428,   376,   136,    72, 19372,    13,    24,   773,\n",
      "           6,    68,   356,  1429, 10813,  1935,    95,    30,     3,\n",
      "           9, 22429,    21,     8,   880,    13,     8,  3742,     6,\n",
      "          11,   243,    24,     3,    88,   225,    59,   281,    91,\n",
      "          12,   577,    21,    24,   471,     5,     1], dtype=int32), 'targets_pretokenized': b'Bill', 'targets': array([3259,    1], dtype=int32), 'label': 1, 'idx': 426}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: Sara borrowed the book from the library because she needs *it* for an article she is working on. She reads it when she gets home from work.', 'inputs': array([    3,   210,     7,    75,    10, 12969, 26688,     8,   484,\n",
      "          45,     8,  3595,   250,   255,   523,  1429,   155,  1935,\n",
      "          21,    46,  1108,   255,    19,   464,    30,     5,   451,\n",
      "         608,     7,    34,   116,   255,  2347,   234,    45,   161,\n",
      "           5,     1], dtype=int32), 'targets_pretokenized': b'the book', 'targets': array([  8, 484,   1], dtype=int32), 'label': 1, 'idx': 334}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: John promised Bill to leave, so an hour later *he* left.', 'inputs': array([    3,   210,     7,    75,    10,  1079, 11130,  3259,    12,\n",
      "        1175,     6,    78,    46,  1781,   865,  1429,    88,  1935,\n",
      "         646,     5,     1], dtype=int32), 'targets_pretokenized': b'John', 'targets': array([1079,    1], dtype=int32), 'label': 1, 'idx': 247}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: Jim comforted Kevin because *he* was so upset.', 'inputs': array([    3,   210,     7,    75,    10,  6006,  2115,    15,    26,\n",
      "        8595,   250,  1429,    88,  1935,    47,    78, 13423,     5,\n",
      "           1], dtype=int32), 'targets_pretokenized': b'Kevin', 'targets': array([8595,    1], dtype=int32), 'label': 1, 'idx': 278}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: When Tatyana reached the cabin, her mother was sleeping. *She* was careful not to disturb her, undressing and climbing back into her berth.', 'inputs': array([    3,   210,     7,    75,    10,   366,  9233, 21247,  3495,\n",
      "           8,  7788,     6,   160,  2039,    47,  9182,     5,  1429,\n",
      "       12736,  1935,    47,  6195,    59,    12, 31738,   160,     6,\n",
      "          64,  9377,    53,    11, 11908,   223,   139,   160,     3,\n",
      "        1152,   189,     5,     1], dtype=int32), 'targets_pretokenized': b'Tatyana', 'targets': array([ 9233, 21247,     1], dtype=int32), 'label': 1, 'idx': 181}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: It is not easy to space buttonholes exactly the same distance apart, and it is very difficult to cut them precisely the right size . The tiniest slip of the scissors will make the hole too large, and even one thread uncut will leave *it* too small.', 'inputs': array([    3,   210,     7,    75,    10,    94,    19,    59,   514,\n",
      "          12,   628,  2218,  9136,     7,  1776,     8,   337,  2357,\n",
      "        3943,     6,    11,    34,    19,   182,  1256,    12,  1340,\n",
      "         135, 11185,     8,   269,   812,     3,     5,    37,     3,\n",
      "          17,    77,    23,   222,  7560,    13,     8, 28958,    56,\n",
      "         143,     8,  6356,   396,   508,     6,    11,   237,    80,\n",
      "        4546,    73,  3044,    56,  1175,  1429,   155,  1935,   396,\n",
      "         422,     5,     1], dtype=int32), 'targets_pretokenized': b'the hole', 'targets': array([   8, 6356,    1], dtype=int32), 'label': 1, 'idx': 314}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: The journalists interviewed the stars of the new movie. *They* were very persistent, so the interview lasted for a long time.', 'inputs': array([    3,   210,     7,    75,    10,    37, 16085, 19257,     8,\n",
      "        4811,    13,     8,   126,  1974,     5,  1429, 10273,  1935,\n",
      "         130,   182, 15803,     6,    78,     8,  2772,     3, 19054,\n",
      "          21,     3,     9,   307,    97,     5,     1], dtype=int32), 'targets_pretokenized': b'The journalists', 'targets': array([   37, 16085,     1], dtype=int32), 'label': 1, 'idx': 55}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b\"wsc: My meeting started at 4:00 and I needed to catch the train at 4:30, so there wasn't much time. Luckily, *it* was delayed, so it worked out.\", 'inputs': array([    3,   210,     7,    75,    10,   499,  1338,   708,    44,\n",
      "       29415,    11,    27,   906,    12,  3579,     8,  2412,    44,\n",
      "       29505,     6,    78,   132,  2088,    31,    17,   231,    97,\n",
      "           5,     3, 16996,     6,  1429,   155,  1935,    47, 16124,\n",
      "           6,    78,    34,  1279,    91,     5,     1], dtype=int32), 'targets_pretokenized': b'the train', 'targets': array([   8, 2412,    1], dtype=int32), 'label': 1, 'idx': 183}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: As Andrea in the crop duster passed over Susan , *she* could see the landing strip.', 'inputs': array([    3,   210,     7,    75,    10,   282, 11813,    16,     8,\n",
      "       10550,   146,  1370,  2804,   147, 10445,     3,     6,  1429,\n",
      "           7,    88,  1935,   228,   217,     8,  9501,  7706,     5,\n",
      "           1], dtype=int32), 'targets_pretokenized': b'Andrea', 'targets': array([11813,     1], dtype=int32), 'label': 1, 'idx': 279}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: Carol believed that Rebecca regretted that *she* had stolen the watch.', 'inputs': array([    3,   210,     7,    75,    10, 12526,  6141,    24, 20570,\n",
      "       31277,    26,    24,  1429,     7,    88,  1935,   141, 14244,\n",
      "           8,  1605,     5,     1], dtype=int32), 'targets_pretokenized': b'Rebecca', 'targets': array([20570,     1], dtype=int32), 'label': 1, 'idx': 57}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: The city councilmen refused the demonstrators a permit because *they* advocated violence.', 'inputs': array([    3,   210,     7,    75,    10,    37,   690,  6098,   904,\n",
      "       12191,     8, 13191,  6230,     3,     9,  5720,   250,  1429,\n",
      "       11056,  1935, 11223,    26,  4756,     5,     1], dtype=int32), 'targets_pretokenized': b'the demonstrators', 'targets': array([    8, 13191,  6230,     1], dtype=int32), 'label': 1, 'idx': 16}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: We went to the lake , because a shark had been seen at the ocean beach , so *it* was a safer place to swim.', 'inputs': array([    3,   210,     7,    75,    10,   101,   877,    12,     8,\n",
      "        6957,     3,     6,   250,     3,     9, 18058,   141,   118,\n",
      "         894,    44,     8,  5431,  2608,     3,     6,    78,  1429,\n",
      "         155,  1935,    47,     3,     9, 14289,   286,    12,  9728,\n",
      "           5,     1], dtype=int32), 'targets_pretokenized': b'the lake', 'targets': array([   8, 6957,    1], dtype=int32), 'label': 1, 'idx': 263}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: Mark told Pete many lies about himself, which Pete included in his book. *He* should have been more skeptical.', 'inputs': array([    3,   210,     7,    75,    10,  2185,  1219, 19786,   186,\n",
      "        7797,    81,  2448,     6,    84, 19786,  1285,    16,   112,\n",
      "         484,     5,  1429,  3845,  1935,   225,    43,   118,    72,\n",
      "       27716,     5,     1], dtype=int32), 'targets_pretokenized': b'Pete', 'targets': array([19786,     1], dtype=int32), 'label': 1, 'idx': 109}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b\"wsc: Billy cried because Toby wouldn't accept *his* toy.\", 'inputs': array([    3,   210,     7,    75,    10, 17724,     3, 24781,   250,\n",
      "         304,   969,  3290,    31,    17,  1845,  1429, 10193,  1935,\n",
      "          12,    63,     5,     1], dtype=int32), 'targets_pretokenized': b'Billy', 'targets': array([17724,     1], dtype=int32), 'label': 1, 'idx': 90}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: This book introduced Shakespeare to Ovid ; it was a fine selection of *his* writing.', 'inputs': array([    3,   210,     7,    75,    10,   100,   484,  3665, 18191,\n",
      "          12,   411,  6961,     3,   117,    34,    47,     3,     9,\n",
      "        1399,  1801,    13,  1429, 10193,  1935,   913,     5,     1],\n",
      "      dtype=int32), 'targets_pretokenized': b'Ovid', 'targets': array([ 411, 6961,    1], dtype=int32), 'label': 1, 'idx': 249}\n",
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: Dan took the rear seat while Bill claimed the front because *his* \"Dibs!\" was quicker.', 'inputs': array([    3,   210,     7,    75,    10,  2744,   808,     8,  4091,\n",
      "        3143,   298,  3259,  7760,     8,   851,   250,  1429, 10193,\n",
      "        1935,    96,   308,    23,   115,     7,  4720,    47, 16403,\n",
      "           5,     1], dtype=int32), 'targets_pretokenized': b'Bill', 'targets': array([3259,    1], dtype=int32), 'label': 1, 'idx': 271}\n"
     ]
    }
   ],
   "source": [
    "cnt=20\n",
    "for i, ex in enumerate(iterator):\n",
    "    print(ex.keys())\n",
    "    print(ex)\n",
    "    cnt-=1\n",
    "    if cnt==0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f8948769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['inputs_pretokenized', 'inputs', 'targets_pretokenized', 'targets', 'label', 'idx'])\n",
      "{'inputs_pretokenized': b'wsc: John hired Bill to take care of *him* .', 'inputs': array([    3,   210,     7,    75,    10,  1079, 10626,  3259,    12,\n",
      "         240,   124,    13,  1429, 10813,  1935,     3,     5,     1],\n",
      "      dtype=int32), 'targets_pretokenized': b'John', 'targets': array([1079,    1], dtype=int32), 'label': 1, 'idx': 461}\n"
     ]
    }
   ],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"super_glue_wsc_axg_n\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "for i, ex in enumerate(iterator):\n",
    "    print(ex.keys())\n",
    "    print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a52f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61641e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"idx\": [],\n",
    "    \"inputs_pretokenized\": [],\n",
    "    \"targets_pretokenized\": []\n",
    "}\n",
    "for i, ex in enumerate(iterator):\n",
    "    dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "    dictionary[\"inputs_pretokenized\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\") )\n",
    "    dictionary[\"targets_pretokenized\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "515b38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a56788a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idx', 'inputs_pretokenized', 'targets_pretokenized']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c75c570a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"mnli hypothesis: You won't learn anything by serving overseas.  premise: because actually when you when you do uh service overseas you end up learning something usually that's that's really useful plumbing or farming or or something like that so you're really learning a skill\",\n",
       " 'mnli hypothesis: This is a stringed instrument created by the ancient Aztecs. premise: The geiro, a percussion instrument made of a notched dried gourd, was developed by the Taano Indians.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['inputs_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3560aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contradiction', 'contradiction']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['targets_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ef46d57c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No Task or Mixture found with name 'glue_wic_v002'. Available:\n - c4_noclean_v020_unsupervised\n - c4_realnewslike_v020_unsupervised\n - c4_v020_unsupervised\n - c4_v220_iid_denoising\n - c4_v220_prefix_lm\n - c4_v220_span_corruption\n - c4_webtextlike_v020_unsupervised\n - cnn_dailymail_v002\n - dpr_v001_simple\n - dpr_v001_simple_1_sentinel\n - glue_ax_v002\n - glue_cola_v002\n - glue_mnli_matched_v002\n - glue_mnli_mismatched_v002\n - glue_mnli_v002\n - glue_mrpc_v002\n - glue_qnli_v002\n - glue_qqp_v002\n - glue_rte_v002\n - glue_sst2_v002\n - glue_stsb_v002\n - glue_wnli_v002\n - glue_wnli_v002_simple_eval\n - squad_v010\n - squad_v010_allanswers\n - squad_v010_allanswers_span\n - squad_v010_context_free\n - super_glue_axb_v102\n - super_glue_axb_v102_1_sentinel\n - super_glue_axg_v102\n - super_glue_axg_v102_1_sentinel\n - super_glue_boolq_v102\n - super_glue_boolq_v102_1_sentinel\n - super_glue_cb_v102\n - super_glue_cb_v102_1_sentinel\n - super_glue_copa_v102\n - super_glue_copa_v102_1_sentinel\n - super_glue_multirc_v102\n - super_glue_multirc_v102_1_sentinel\n - super_glue_record_v102\n - super_glue_record_v102_1_sentinel\n - super_glue_rte_v102\n - super_glue_rte_v102_1_sentinel\n - super_glue_wic_v102\n - super_glue_wic_v102_1_sentinel\n - super_glue_wsc_v102_simple_1_sentinel_eval\n - super_glue_wsc_v102_simple_1_sentinel_train\n - super_glue_wsc_v102_simple_eval\n - super_glue_wsc_v102_simple_train\n - trivia_qa_v010\n - wikipedia_20190301.en_v003_unsupervised\n - wmt14_ende_v003\n - wmt14_enfr_v003\n - wmt15_enfr_v003\n - wmt16_enro_v003\n - wmt19_ende_v003\n - wmt_t2t_ende_v003",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1282/4092235101.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#glue_mnli_v002\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#super_glue_cb_v102\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m dataset = seqio.get_mixture_or_task(\"glue_wic_v002\").get_dataset(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"inputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"targets\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/seqio/dataset_providers.py\u001b[0m in \u001b[0;36mget_mixture_or_task\u001b[0;34m(task_or_mixture_name)\u001b[0m\n\u001b[1;32m   1515\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mTaskRegistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_or_mixture_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1517\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m   1518\u001b[0m         \u001b[0;34m\"No Task or Mixture found with name '%s'. Available:\\n - %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m         (task_or_mixture_name, \"\\n - \".join(sorted(tasks))))\n",
      "\u001b[0;31mValueError\u001b[0m: No Task or Mixture found with name 'glue_wic_v002'. Available:\n - c4_noclean_v020_unsupervised\n - c4_realnewslike_v020_unsupervised\n - c4_v020_unsupervised\n - c4_v220_iid_denoising\n - c4_v220_prefix_lm\n - c4_v220_span_corruption\n - c4_webtextlike_v020_unsupervised\n - cnn_dailymail_v002\n - dpr_v001_simple\n - dpr_v001_simple_1_sentinel\n - glue_ax_v002\n - glue_cola_v002\n - glue_mnli_matched_v002\n - glue_mnli_mismatched_v002\n - glue_mnli_v002\n - glue_mrpc_v002\n - glue_qnli_v002\n - glue_qqp_v002\n - glue_rte_v002\n - glue_sst2_v002\n - glue_stsb_v002\n - glue_wnli_v002\n - glue_wnli_v002_simple_eval\n - squad_v010\n - squad_v010_allanswers\n - squad_v010_allanswers_span\n - squad_v010_context_free\n - super_glue_axb_v102\n - super_glue_axb_v102_1_sentinel\n - super_glue_axg_v102\n - super_glue_axg_v102_1_sentinel\n - super_glue_boolq_v102\n - super_glue_boolq_v102_1_sentinel\n - super_glue_cb_v102\n - super_glue_cb_v102_1_sentinel\n - super_glue_copa_v102\n - super_glue_copa_v102_1_sentinel\n - super_glue_multirc_v102\n - super_glue_multirc_v102_1_sentinel\n - super_glue_record_v102\n - super_glue_record_v102_1_sentinel\n - super_glue_rte_v102\n - super_glue_rte_v102_1_sentinel\n - super_glue_wic_v102\n - super_glue_wic_v102_1_sentinel\n - super_glue_wsc_v102_simple_1_sentinel_eval\n - super_glue_wsc_v102_simple_1_sentinel_train\n - super_glue_wsc_v102_simple_eval\n - super_glue_wsc_v102_simple_train\n - trivia_qa_v010\n - wikipedia_20190301.en_v003_unsupervised\n - wmt14_ende_v003\n - wmt14_enfr_v003\n - wmt15_enfr_v003\n - wmt16_enro_v003\n - wmt19_ende_v003\n - wmt_t2t_ende_v003"
     ]
    }
   ],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"glue_mnli_v002\").get_dataset(\n",
    "    sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88f2119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"idx\": [],\n",
    "    \"inputs_pretokenized\": [],\n",
    "    \"targets_pretokenized\": []\n",
    "}\n",
    "for i, ex in enumerate(iterator):\n",
    "    dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "    dictionary[\"inputs_pretokenized\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\") )\n",
    "    dictionary[\"targets_pretokenized\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e513de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05108dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"mnli hypothesis: You won't learn anything by serving overseas.  premise: because actually when you when you do uh service overseas you end up learning something usually that's that's really useful plumbing or farming or or something like that so you're really learning a skill\",\n",
       " 'mnli hypothesis: This is a stringed instrument created by the ancient Aztecs. premise: The geiro, a percussion instrument made of a notched dried gourd, was developed by the Taano Indians.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['inputs_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8542c0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contradiction', 'contradiction']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['targets_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55f31010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 3.93 MiB (download: 3.93 MiB, generated: Unknown size, total: 3.93 MiB) to /root/tensorflow_datasets/super_glue/boolq/1.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421891c4ab914cb7a82c067b567d1116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ac117c1b934e5f91a5b4c9f63bdbee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245a39afcc6c42258fa075959a2cd490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32c5308ca4f4ccea0b5fdfd4040553f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b59246b90c55403193e87a87348c72e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/9427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0230b43d710f4cff8d3719010890c51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteCOOR2N/super_glue-train.tfrecord*...:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a816b418c1434067889c796cc1339f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961f3746021b438eb39810534a7fa0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteCOOR2N/super_glue-validation.tfrecord*...…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84e0f6c9cc543a3b56c05d397b8f12e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/3245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "860d38ab3fc04abea654b2ea10c53188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling /root/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteCOOR2N/super_glue-test.tfrecord*...:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset super_glue downloaded and prepared to /root/tensorflow_datasets/super_glue/boolq/1.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dataset = seqio.get_mixture_or_task(\"super_glue_boolq_v102\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df17d26",
   "metadata": {},
   "source": [
    "# Create dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53e3b21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": [],\n",
    "    }\n",
    "    \n",
    "    if name == 'record':\n",
    "        dictionary['answers']=[]\n",
    "\n",
    "    for i, ex in enumerate(iterator):\n",
    "        if name =='multirc':\n",
    "            dictionary[\"idx\"].append(f\"{ex['idx/paragraph']}-{ex['idx/question']}-{ex['idx/answer']}\") \n",
    "        elif name == 'record':\n",
    "            dictionary[\"idx\"].append(ex[\"idx/query\"])\n",
    "            dictionary['answers'].append([i.decode(\"utf-8\") for i in ex[\"answers\"]])\n",
    "        else: dictionary[\"idx\"].append(ex[\"idx\"]) \n",
    "\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3a3a12",
   "metadata": {},
   "source": [
    "## for mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7eb7a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3d816984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test_matched\n",
      "test_mismatched\n",
      "validation_matched\n",
      "validation_mismatched\n"
     ]
    }
   ],
   "source": [
    "name=\"mnli\"\n",
    "#super_glue_cb_v102\n",
    "for split in ['train','test_matched','test_mismatched','validation_matched','validation_mismatched']:\n",
    "    print(split)\n",
    "    seqio_mnli_dataset[split]=get_date(name,split,'glue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f78dddde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4019b968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation and test dataset by combining matched and mismatched\n",
    "\n",
    "seqio_mnli_dataset['validation']=concatenate_datasets([seqio_mnli_dataset['validation_matched'],seqio_mnli_dataset['validation_mismatched']])\n",
    "seqio_mnli_dataset['test']=concatenate_datasets([seqio_mnli_dataset['test_matched'],seqio_mnli_dataset['test_mismatched']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1bbc2361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19647\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'inputs', 'targets'],\n",
       "        num_rows: 19643\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dfd6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset.push_to_hub(\n",
    "        \"stjokerli/TextToText_mnli_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a256c69",
   "metadata": {},
   "source": [
    "## For superGlue\n",
    "\n",
    "### For 'cb','boolq','rte','copa' ,'wic','multirc','record'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dfe3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6a0169b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "record\n",
      "  train\n",
      "  test\n",
      "  validation\n"
     ]
    }
   ],
   "source": [
    "# task_dict=\n",
    "for name in [\n",
    "#     'cb','boolq','rte','copa'\n",
    "#             ,'wic',\n",
    "#     'multirc',\n",
    "    'record'\n",
    "            ]:\n",
    "   \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['train','test','validation']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739bad02",
   "metadata": {},
   "source": [
    "## For wcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69df5268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name,subfix,split):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "#     if dataset_name=='superglue':\n",
    "#         seqio_name=f\"super_glue_{name}_v102\"\n",
    "#     elif dataset_name=='glue':\n",
    "#         seqio_name=f\"glue_{name}_v002\"\n",
    "#     else :\n",
    "#         raise f\"dataset_name: {dataset_name} not config\"\n",
    "    seqio_name=f\"super_glue_{name}_v102_simple_{subfix}\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "        dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31e2ca7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=DatasetDict()\n",
    "temp['train']=get_data('wsc','train','train')\n",
    "temp['validation']=get_data('wsc','eval','validation')\n",
    "temp['test']=get_data('wsc','eval','test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2775e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue['wsc']=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c4ea0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split,dataset_name):\n",
    "    \n",
    "    start_position=len(name)+1\n",
    "    \n",
    "    if dataset_name=='superglue':\n",
    "        seqio_name=f\"super_glue_{name}_v102\"\n",
    "    elif dataset_name=='glue':\n",
    "        seqio_name=f\"glue_{name}_v002\"\n",
    "    else :\n",
    "        raise f\"dataset_name: {dataset_name} not config\"\n",
    "    dataset = seqio.get_mixture_or_task(seqio_name).get_dataset(\n",
    "        sequence_length={\"inputs\": 1, \"targets\": 1},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"inputs\": [],\n",
    "        \"targets\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "        dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"inputs\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\")[start_position:])\n",
    "#         print(ex[\"targets_pretokenized\"])\n",
    "        dictionary[\"targets\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "549de99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axg\n",
      "  test\n",
      "axb\n",
      "  test\n"
     ]
    }
   ],
   "source": [
    "for name in ['axg','axb']:\n",
    "    \n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['test']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(name,split,\"superglue\")\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "546125b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    cb: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 56\n",
       "        })\n",
       "    })\n",
       "    boolq: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 9427\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3245\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3270\n",
       "        })\n",
       "    })\n",
       "    rte: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 3000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "    copa: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 400\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 500\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 100\n",
       "        })\n",
       "    })\n",
       "    wic: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 5428\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1400\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 638\n",
       "        })\n",
       "    })\n",
       "    multirc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 27243\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 9693\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 4848\n",
       "        })\n",
       "    })\n",
       "    record: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 138854\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 10000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 15176\n",
       "        })\n",
       "    })\n",
       "    wsc: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 259\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 104\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 146\n",
       "        })\n",
       "    })\n",
       "    axg: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 356\n",
       "        })\n",
       "    })\n",
       "    axb: DatasetDict({\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets'],\n",
       "            num_rows: 1104\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a489587",
   "metadata": {},
   "source": [
    "# push to hugging face data hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c35c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90362c89c5d343fc83c862c1921be56e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e262ddf7dc8b472181efe6e359f63326",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccda9a4993af4191a34e56b93df71ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "    text_to_text_SuperGlue[task].push_to_hub(\n",
    "        f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d2a76e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    record: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets', 'answers'],\n",
       "            num_rows: 138854\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets', 'answers'],\n",
       "            num_rows: 10000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'inputs', 'targets', 'answers'],\n",
       "            num_rows: 15176\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60555581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
