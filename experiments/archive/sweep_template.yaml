program: run_sweep.py
method: bayes
metric:
  name: eval/accuracy
  goal: maximize
parameters:
  ffn_bn:
    distribution: int_uniform
    min: 256
    max: 1024
  attn_bn:
    distribution: int_uniform
    min: 100
    max: 400
  d_model:
    distribution: int_uniform
    min: 512
    max: 2048
  dropout:
    distribution: uniform
    min: 0.05
    max: 0.2
  mid_dim:
    distribution: int_uniform
    min: 400
    max: 1600
  ffn_mode:
    distribution: categorical
    values:
      - none
  adafactor:
    distribution: categorical
    values:
      - "true"
      - "false"
  attn_mode:
    distribution: categorical
    values:
      - prefix
  deepspeed:
    distribution: categorical
    values:
      - None
  do_sample:
    distribution: categorical
    values:
      - "true"
      - "false"
  load_path:
    distribution: categorical
    values:
      - ""
  lora_init:
    distribution: categorical
    values:
      - lora
  max_steps:
    distribution: int_uniform
    min: 2500
    max: 524288
  num_beams:
    distribution: int_uniform
    min: 2
    max: 8
  adam_beta1:
    distribution: uniform
    min: 0.45
    max: 1.8
  adam_beta2:
    distribution: uniform
    min: 0.4995
    max: 1.998
  ffn_option:
    distribution: categorical
    values:
      - parallel
  lora_alpha:
    distribution: int_uniform
    min: 16
    max: 64
  max_length:
    distribution: int_uniform
    min: 10
    max: 40
  attn_option:
    distribution: categorical
    values:
      - concat
  adam_epsilon:
    distribution: uniform
    min: 5e-9
    max: 2e-8
  warmup_steps:
    distribution: int_uniform
    min: 0
    max: 2000
  weight_decay:
    distribution: uniform
    min: 0
    max: 0.02
  gen_num_beams:
    distribution: int_uniform
    min: 3
    max: 12
  learning_rate:
    distribution: uniform
    min: 0.0000025
    max: 0.0001
  logging_steps:
    distribution: int_uniform
    min: 5
    max: 10000
  max_grad_norm:
    distribution: uniform
    min: 0.05
    max: 0.2
  decoder_layers:
    distribution: int_uniform
    min: 6
    max: 24
  encoder_layers:
    distribution: int_uniform
    min: 6
    max: 24
  decoder_ffn_dim:
    distribution: int_uniform
    min: 2048
    max: 8192
  encoder_ffn_dim:
    distribution: int_uniform
    min: 2048
    max: 8192
  attn_composition:
    distribution: categorical
    values:
      - add
  num_train_epochs:
    distribution: int_uniform
    min: 6
    max: 20000
  train_batch_size:
    distribution: int_uniform
    min: 8
    max: 64
  attention_dropout:
    distribution: uniform
    min: 0.05
    max: 0.2
  lr_scheduler_type:
    distribution: categorical
    values:
      - polynomial
  max_source_length:
    distribution: int_uniform
    min: 256
    max: 1024
  max_target_length:
    distribution: int_uniform
    min: 64
    max: 256
  num_hidden_layers:
    distribution: int_uniform
    min: 6
    max: 24
  activation_dropout:
    distribution: uniform
    min: 0.05
    max: 0.2
  ffn_adapter_scalar:
    distribution: categorical
    values:
      - "4"
  no_repeat_ngram_size:
    distribution: int_uniform
    min: 2
    max: 6
  label_smoothing_factor:
    distribution: uniform
    min: 0.05
    max: 0.2
  ffn_adapter_init_option:
    distribution: categorical
    values:
      - lora
  gradient_accumulation_steps:
    distribution: int_uniform
    min: 1
    max: 4
  ffn_adapter_layernorm_option:
    distribution: categorical
    values:
      - none
