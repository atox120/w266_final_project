{
    "attn_bn": 110,
    "attn_composition": "add",
    "attn_mode": "prompt_tuning",
    "attn_option": "parallel",
    "cache_dir": "/workspace/w266_final_project/dataset_checkpoints/PETL_model",
    "dataset_name": "stjokerli/TextToText_cb_seqio",
    "disable_tqdm": true,
    "do_eval": true,
    "do_predict": true,
    "do_train": true,
    "eval_steps": 1500,
    "evaluation_strategy": "steps",
    "ffn_adapter_init_option": "lora",
    "ffn_adapter_layernorm_option": "none",
    "ffn_adapter_scalar": "4",
    "ffn_bn": 512,
    "ffn_mode": "none",
    "ffn_option": "parallel",
    "fp16": true,
    "gradient_accumulation_steps": 2,
    "greater_is_better": true,
    "label_smoothing_factor": 0.1,
    "learning_rate": 1.3999999999999998e-05,
    "load_best_model_at_end": true,
    "load_path": "/workspace/w266_final_project/model_checkpoints/offical_experiments_checkpoints/Prompt_target_tunning/source_model/checkpoint-225000",
    "logging_steps": 100,
    "lr_scheduler_type": "polynomial",
    "max_eval_samples": 1600,
    "max_grad_norm": 0.1,
    "max_length": 60,
    "max_source_length": 512,
    "max_steps": 100000,
    "max_target_length": 128,
    "metric_for_best_model": "accuracy",
    "mid_dim": 800,
    "min_length": 1,
    "model_name_or_path": "facebook/bart-large",
    "model_or_path": "/workspace/w266_final_project/model_checkpoints/offical_experiments_checkpoints/Prompt_target_tunning/source_model/checkpoint-225000",
    "no_repeat_ngram_size": 3,
    "num_beams": 5,
    "num_train_epochs": 30,
    "output_dir": "/workspace/w266_final_project/model_checkpoints/offical_experiments_checkpoints/Prompt_target_tunning/target_transfer_learnings/Prompt_target_transfer_learnings_cb",
    "overwrite_output_dir": true,
    "per_device_eval_batch_size": 16,
    "per_device_train_batch_size": 16,
    "predict_with_generate": true,
    "preprocessing_num_workers": 4,
    "project": "Official_experiment",
    "report_to": "wandb",
    "resume_from_checkpoint": "/workspace/w266_final_project/model_checkpoints/offical_experiments_checkpoints/Prompt_target_tunning/source_model/checkpoint-225000",
    "run_name": "Prompt_target_transfer_learnings_cb",
    "save_steps": 1500,
    "save_strategy": "steps",
    "save_total_limit": 2,
    "unfreeze_params": "ef_",
    "val_max_target_length": 10,
    "warmup_steps": 200,
    "weight_decay": 0.01
}