{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe36ee6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: seqio in /opt/conda/lib/python3.8/site-packages (0.0.7)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.8/site-packages (from seqio) (1.0.0)\n",
      "Requirement already satisfied: tensorflow-text in /opt/conda/lib/python3.8/site-packages (from seqio) (2.8.1)\n",
      "Requirement already satisfied: tfds-nightly in /opt/conda/lib/python3.8/site-packages (from seqio) (4.5.2.dev202203010044)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from seqio) (1.22.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (from seqio) (0.1.96)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from absl-py->seqio) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-text->seqio) (0.12.0)\n",
      "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-text->seqio) (2.8.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (3.19.3)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (1.1.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (5.4.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (4.62.3)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (0.3.4)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (0.10.2)\n",
      "Requirement already satisfied: etils[epath-no-tf] in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (2.26.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (1.7.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->seqio) (2.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tfds-nightly->seqio) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tfds-nightly->seqio) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tfds-nightly->seqio) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests>=2.19.0->tfds-nightly->seqio) (2.0.9)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (0.2.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (1.37.1)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (0.5.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (0.24.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (1.6.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (4.0.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (1.1.2)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (3.6.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (59.5.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (2.0)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (2.8.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (13.0.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (1.13.3)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.8/site-packages (from etils[epath-no-tf]->tfds-nightly->seqio) (3.7.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-metadata->tfds-nightly->seqio) (1.55.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (0.37.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (2.4.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (0.16.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (0.4.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (4.10.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->seqio) (3.1.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting t5\n",
      "  Downloading t5-0.9.3-py3-none-any.whl (153 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: transformers>=2.7.0 in /opt/conda/lib/python3.8/site-packages (from t5) (4.6.1)\n",
      "Collecting mesh-tensorflow[transformer]>=0.1.13\n",
      "  Downloading mesh_tensorflow-0.1.19-py3-none-any.whl (366 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.4/366.4 KB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rouge-score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Collecting editdistance\n",
      "  Downloading editdistance-0.6.0-cp38-cp38-manylinux2010_x86_64.whl (286 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.4/286.4 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: babel in /opt/conda/lib/python3.8/site-packages (from t5) (2.9.1)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.8/site-packages (from t5) (1.10.2+cu113)\n",
      "Requirement already satisfied: seqio in /opt/conda/lib/python3.8/site-packages (from t5) (0.0.7)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from t5) (1.6.3)\n",
      "Requirement already satisfied: six>=1.14 in /opt/conda/lib/python3.8/site-packages (from t5) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (from t5) (0.1.96)\n",
      "Collecting gin-config\n",
      "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 KB\u001b[0m \u001b[31m68.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from t5) (1.3.5)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from t5) (1.22.0)\n",
      "Requirement already satisfied: tfds-nightly in /opt/conda/lib/python3.8/site-packages (from t5) (4.5.2.dev202203010044)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from t5) (0.24.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.8/site-packages (from t5) (1.0.0)\n",
      "Requirement already satisfied: tensorflow-text in /opt/conda/lib/python3.8/site-packages (from t5) (2.8.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (from t5) (3.6.7)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from mesh-tensorflow[transformer]>=0.1.13->t5) (0.18.2)\n",
      "Collecting tensorflow-datasets\n",
      "  Downloading tensorflow_datasets-4.5.2-py3-none-any.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5) (3.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5) (4.62.3)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5) (0.0.47)\n",
      "Collecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5) (2022.1.18)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5) (2.26.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5) (21.3)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /opt/conda/lib/python3.8/site-packages (from transformers>=2.7.0->t5) (0.10.3)\n",
      "Requirement already satisfied: pytz>=2015.7 in /opt/conda/lib/python3.8/site-packages (from babel->t5) (2021.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from nltk->t5) (1.1.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk->t5) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas->t5) (2.8.2)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from sacrebleu->t5) (0.4.4)\n",
      "Collecting portalocker\n",
      "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.8/site-packages (from sacrebleu->t5) (0.8.9)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->t5) (3.0.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-text->t5) (0.12.0)\n",
      "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-text->t5) (2.8.0)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5) (0.3.4)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5) (1.1.0)\n",
      "Requirement already satisfied: promise in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5) (2.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5) (1.7.0)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5) (0.10.2)\n",
      "Requirement already satisfied: importlib-resources in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5) (5.4.0)\n",
      "Requirement already satisfied: etils[epath-no-tf] in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5) (0.4.0)\n",
      "Requirement already satisfied: protobuf>=3.12.2 in /opt/conda/lib/python3.8/site-packages (from tfds-nightly->t5) (3.19.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.8/site-packages (from torch->t5) (4.0.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers>=2.7.0->t5) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers>=2.7.0->t5) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers>=2.7.0->t5) (3.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers>=2.7.0->t5) (2.0.9)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (2.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.2.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.13.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (59.5.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (2.8.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (3.6.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (13.0.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.24.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.37.1)\n",
      "Requirement already satisfied: gast>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.5.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.6.3)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.8/site-packages (from etils[epath-no-tf]->tfds-nightly->t5) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->transformers>=2.7.0->t5) (3.0.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow-metadata->tfds-nightly->t5) (1.55.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.37.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (2.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.16.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.8.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (4.10.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (3.1.1)\n",
      "Installing collected packages: gin-config, portalocker, editdistance, sacrebleu, rouge-score, mesh-tensorflow, huggingface-hub, tensorflow-datasets, t5\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.4.0\n",
      "    Uninstalling huggingface-hub-0.4.0:\n",
      "      Successfully uninstalled huggingface-hub-0.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "farm 0.8.0 requires torch<1.9,>1.5, but you have torch 1.10.2+cu113 which is incompatible.\n",
      "datasets 1.18.3 requires huggingface-hub<1.0.0,>=0.1.0, but you have huggingface-hub 0.0.8 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed editdistance-0.6.0 gin-config-0.5.0 huggingface-hub-0.0.8 mesh-tensorflow-0.1.19 portalocker-2.4.0 rouge-score-0.0.4 sacrebleu-2.0.0 t5-0.9.3 tensorflow-datasets-4.5.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install seqio\n",
    "!pip install t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790a7ffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5c1c01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85f01d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqio\n",
    "import functools\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "import t5.data\n",
    "from t5.data import postprocessors\n",
    "from t5.data import preprocessors\n",
    "from t5.data.glue_utils import get_glue_metric\n",
    "from t5.data.glue_utils import get_glue_postprocess_fn\n",
    "from t5.data.glue_utils import get_glue_text_preprocessor\n",
    "from t5.data.glue_utils import get_super_glue_metric\n",
    "from t5.evaluation import metrics\n",
    "import tensorflow_datasets as tfds\n",
    "from t5.models import utils as model_utils\n",
    "import gin\n",
    "from absl import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cff5282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.5.2+nightly'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfds.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc4ddea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seqio.dataset_providers.Task at 0x7fcd1f98acd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TaskRegistry = seqio.TaskRegistry\n",
    "\n",
    "\n",
    "\n",
    "DEFAULT_OUTPUT_FEATURES = {\n",
    "    \"inputs\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True,\n",
    "        required=False),\n",
    "    \"targets\": seqio.Feature(\n",
    "        vocabulary=t5.data.get_default_vocabulary(), add_eos=True)\n",
    "}\n",
    "\n",
    "# ==================================== C4 ======================================\n",
    "# Final pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_span_corruption\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.span_corruption,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Baseline pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_iid_denoising\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.iid_denoising,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Prefix language modeling pretraining task used in Raffel et al., 2019.\n",
    "TaskRegistry.add(\n",
    "    \"c4_v220_prefix_lm\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.prefix_lm,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# Configurable tasks used for comparisons in Raffel et al., 2019.\n",
    "_c4_config_suffixes = [\"\", \".noclean\", \".realnewslike\", \".webtextlike\"]\n",
    "for config_suffix in _c4_config_suffixes:\n",
    "    TaskRegistry.add(\n",
    "        \"c4{name}_v020_unsupervised\".format(name=config_suffix.replace(\".\", \"_\")),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"c4/en{config}:2.2.0\".format(\n",
    "          config=config_suffix)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.rekey, key_map={\n",
    "                  \"inputs\": None,\n",
    "                  \"targets\": \"text\"\n",
    "              }),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          preprocessors.unsupervised,\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        metric_fns=[])\n",
    "\n",
    "\n",
    "# ================================ Wikipedia ===================================\n",
    "TaskRegistry.add(\n",
    "    \"wikipedia_20190301.en_v003_unsupervised\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wikipedia/20190301.en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.rekey, key_map={\n",
    "                \"inputs\": None,\n",
    "                \"targets\": \"text\"\n",
    "            }),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.unsupervised,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "    metric_fns=[])\n",
    "\n",
    "\n",
    "# =================================== GLUE =====================================\n",
    "for b in tfds.text.glue.Glue.builder_configs.values():\n",
    "    TaskRegistry.add(\n",
    "        \"glue_%s_v002\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/%s:2.0.0\" % b.name,\n",
    "        splits=[\"test\"] if b.name == \"ax\" else None),\n",
    "        preprocessors=[\n",
    "        get_glue_text_preprocessor(b),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=get_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "# =============================== CNN DailyMail ================================\n",
    "TaskRegistry.add(\n",
    "    \"cnn_dailymail_v002\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"cnn_dailymail:3.1.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.summarize,\n",
    "            article_key=\"article\",\n",
    "            summary_key=\"highlights\"),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.rouge],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ==================================== WMT =====================================\n",
    "# Format: year, tfds builder config, tfds version\n",
    "b_configs = [\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"14\", tfds.translate.wmt14.Wmt14Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"16\", tfds.translate.wmt16.Wmt16Translate.builder_configs[\"ro-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"15\", tfds.translate.wmt15.Wmt15Translate.builder_configs[\"fr-en\"], \"1.0.0\"\n",
    "    ),\n",
    "    (\"19\", tfds.translate.wmt19.Wmt19Translate.builder_configs[\"de-en\"], \"1.0.0\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "for prefix, b, tfds_version in b_configs:\n",
    "    TaskRegistry.add(\n",
    "        \"wmt%s_%s%s_v003\" % (prefix, b.language_pair[1], b.language_pair[0]),\n",
    "        source=seqio.TfdsDataSource(tfds_name=\"wmt%s_translate/%s:%s\" %\n",
    "                                  (prefix, b.name, tfds_version)),\n",
    "        preprocessors=[\n",
    "          functools.partial(\n",
    "              preprocessors.translate,\n",
    "              source_language=b.language_pair[1],\n",
    "              target_language=b.language_pair[0],\n",
    "          ),\n",
    "          seqio.preprocessors.tokenize,\n",
    "          seqio.CacheDatasetPlaceholder(),\n",
    "          seqio.preprocessors.append_eos_after_trim,\n",
    "        ],\n",
    "        metric_fns=[metrics.bleu],\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Special case for t2t ende.\n",
    "b = tfds.translate.wmt_t2t.WmtT2tTranslate.builder_configs[\"de-en\"]\n",
    "TaskRegistry.add(\n",
    "    \"wmt_t2t_ende_v003\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"wmt_t2t_translate/de-en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.translate,\n",
    "            source_language=b.language_pair[1],\n",
    "            target_language=b.language_pair[0]),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.bleu],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= SuperGlue ==================================\n",
    "for b in tfds.text.super_glue.SuperGlue.builder_configs.values():\n",
    "  # We use a simplified version of WSC, defined below\n",
    "    if \"wsc\" in b.name:\n",
    "        continue\n",
    "    if b.name == \"axb\":\n",
    "        glue_preprocessors = [\n",
    "            functools.partial(\n",
    "                preprocessors.rekey,\n",
    "                key_map={\n",
    "                    \"premise\": \"sentence1\",\n",
    "                    \"hypothesis\": \"sentence2\",\n",
    "                    \"label\": \"label\",\n",
    "                    \"idx\": \"idx\",\n",
    "                }),\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "        ]\n",
    "    else:\n",
    "        glue_preprocessors = [\n",
    "            get_glue_text_preprocessor(b),\n",
    "            seqio.preprocessors.tokenize,\n",
    "            seqio.CacheDatasetPlaceholder(),\n",
    "            seqio.preprocessors.append_eos_after_trim,\n",
    "    ]\n",
    "    TaskRegistry.add(\n",
    "        \"super_glue_%s_v102\" % b.name,\n",
    "        source=seqio.TfdsDataSource(\n",
    "          tfds_name=\"super_glue/%s:1.0.2\" % b.name,\n",
    "          splits=[\"test\"] if b.name in [\"axb\", \"axg\"] else None),\n",
    "        preprocessors=glue_preprocessors,\n",
    "        metric_fns=get_super_glue_metric(b.name),\n",
    "        output_features=DEFAULT_OUTPUT_FEATURES,\n",
    "        postprocess_fn=get_glue_postprocess_fn(b))\n",
    "\n",
    "    # Create SuperGLUE tasks with 1 sentinel token added.\n",
    "    seqio.experimental.add_task_with_sentinels(\"super_glue_%s_v102\" % b.name,\n",
    "                                             num_sentinels=1)\n",
    "\n",
    "# ======================== Definite Pronoun Resolution =========================\n",
    "TaskRegistry.add(\n",
    "    \"dpr_v001_simple\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"definite_pronoun_resolution:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.definite_pronoun_resolution_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"dpr_v001_simple\", num_sentinels=1)\n",
    "\n",
    "# =================================== WSC ======================================\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_train\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"train\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.wsc_simple, correct_referent_only=True),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_train\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "TaskRegistry.add(\n",
    "    \"super_glue_wsc_v102_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"super_glue/wsc.fixed:1.0.2\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        functools.partial(\n",
    "            preprocessors.wsc_simple, correct_referent_only=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "# Create SuperGLUE tasks with 1 sentinel token added.\n",
    "seqio.experimental.add_task_with_sentinels(\"super_glue_wsc_v102_simple_eval\",\n",
    "                                           num_sentinels=1)\n",
    "\n",
    "# =================================== WNLI =====================================\n",
    "TaskRegistry.add(\n",
    "    \"glue_wnli_v002_simple_eval\",\n",
    "    source=seqio.TfdsDataSource(\n",
    "        tfds_name=\"glue/wnli:1.0.0\", splits=[\"validation\", \"test\"]),\n",
    "    preprocessors=[\n",
    "        preprocessors.wnli_simple,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.wsc_simple,\n",
    "    metric_fns=[metrics.accuracy],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# =================================== Squad ====================================\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# Maximized evaluation metrics over all answers.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_context_free\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        functools.partial(preprocessors.squad, include_context=False),\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.qa,\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Squad span prediction task instead of text.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010_allanswers_span\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad_span_space_tokenized,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    postprocess_fn=postprocessors.span_qa,\n",
    "    metric_fns=[metrics.span_squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# Deprecated: Use `squad_v010_allanswers` instead.\n",
    "TaskRegistry.add(\n",
    "    \"squad_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"squad/v1.1:3.0.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.squad,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[metrics.squad],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "# ================================= TriviaQA ===================================\n",
    "TaskRegistry.add(\n",
    "    \"trivia_qa_v010\",\n",
    "    source=seqio.TfdsDataSource(tfds_name=\"trivia_qa/rc:1.1.0\"),\n",
    "    preprocessors=[\n",
    "        preprocessors.trivia_qa,\n",
    "        seqio.preprocessors.tokenize,\n",
    "        seqio.CacheDatasetPlaceholder(),\n",
    "        preprocessors.trivia_qa_truncate_inputs,\n",
    "        seqio.preprocessors.append_eos_after_trim,\n",
    "    ],\n",
    "    metric_fns=[],\n",
    "    output_features=DEFAULT_OUTPUT_FEATURES)\n",
    "\n",
    "\n",
    "# =============== PrefixLM objectives (not used in the T5 paper) ===============\n",
    "\n",
    "\n",
    "# # Vocabulary (shared by encoder and decoder)\n",
    "# sentencepiece_model_file = \"gs://t5-data/vocabs/cc_all.32000.100extra/sentencepiece.model\"\n",
    "\n",
    "# vocab = seqio.SentencePieceVocabulary(sentencepiece_model_file)\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_encoder_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_encoder_decoder,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"encoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"encoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_segment_ids\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_positions\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# seqio.TaskRegistry.add(\n",
    "#     \"c4_prefix_lm_objective_decoder_architecture\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.targets_for_prefix_lm_objective,\n",
    "#         preprocessors.pack_prefix_lm_decoder_only,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"decoder_target_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_input_tokens\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_loss_weights\": seqio.Feature(vocabulary=vocab, add_eos=False),\n",
    "#         \"decoder_causal_attention\": seqio.Feature(\n",
    "#             vocabulary=vocab, add_eos=False),\n",
    "#         # All but the last stage of the preprocessing uses \"targets\" as the key,\n",
    "#         # so this output feature is necessary. It is not marked required because\n",
    "#         # the final preprocessor drops it.\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, required=False),\n",
    "#     },\n",
    "#     metric_fns=[])\n",
    "\n",
    "\n",
    "# TaskRegistry.add(\n",
    "#     \"c4_v220_full_lm\",\n",
    "#     source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "#     preprocessors=[\n",
    "#         functools.partial(\n",
    "#             preprocessors.rekey, key_map={\n",
    "#                 \"inputs\": None,\n",
    "#                 \"targets\": \"text\"\n",
    "#             }),\n",
    "#         seqio.preprocessors.tokenize,\n",
    "#         seqio.CacheDatasetPlaceholder(),\n",
    "#         preprocessors.full_lm,\n",
    "#     ],\n",
    "#     output_features={\n",
    "#         \"targets\": seqio.Feature(vocabulary=vocab, add_eos=True)\n",
    "#     },\n",
    "#     metric_fns=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f9c5604",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 12:25:59.831226: W tensorflow/core/platform/cloud/google_auth_provider.cc:184] All attempts to get a Google authentication bearer token failed, returning an empty token. Retrieving token from files failed with \"NOT_FOUND: Could not locate the credentials file.\". Retrieving token from GCE failed with \"FAILED_PRECONDITION: Error executing an HTTP request: libcurl code 6 meaning 'Couldn't resolve host name', error details: Could not resolve host: metadata\".\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 298.29 MiB (download: 298.29 MiB, generated: 100.56 MiB, total: 398.85 MiB) to ~/tensorflow_datasets/glue/mnli/2.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ec9f4d3f4047968d7cde2b04e9b679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "282af118ac324189a054b031dd6746ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bcc023d3754f0eb2abf4c92d92268b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f568d9b7d32a41168b2dc0f14a524794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/5 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006df4f968214720bad5083f3c6cb9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/392702 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e483d58d243942ee95ccde13ff19574a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/glue/mnli/2.0.0.incompleteGRCOYK/glue-train.tfrecord*...:   0%|          | 0/3…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efba45f3232a414cad3a984dce1d1fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_matched examples...:   0%|          | 0/9815 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909dc1183cbb42f4a8219cc00fa13b95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/glue/mnli/2.0.0.incompleteGRCOYK/glue-validation_matched.tfrecord*...:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1c9e65af2b4939a23c756627e2b800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation_mismatched examples...:   0%|          | 0/9832 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e02dd5a8e39e4bf4b1ec299d49147f54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/glue/mnli/2.0.0.incompleteGRCOYK/glue-validation_mismatched.tfrecord*...:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "796b695c5fd54b5e89be53c6eca6f3e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_matched examples...:   0%|          | 0/9796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5748b697632b466c82999a4df8f3934a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/glue/mnli/2.0.0.incompleteGRCOYK/glue-test_matched.tfrecord*...:   0%|        …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b8cbc1bbcf4867997fd9dae3d0e379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test_mismatched examples...:   0%|          | 0/9847 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2362446da1450b945ea3a534f13a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/glue/mnli/2.0.0.incompleteGRCOYK/glue-test_mismatched.tfrecord*...:   0%|     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset glue downloaded and prepared to ~/tensorflow_datasets/glue/mnli/2.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 12:26:52.897581: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-01 12:26:52.961094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-01 12:26:52.961414: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-01 12:26:52.961773: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-01 12:26:52.966123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-01 12:26:52.966365: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-01 12:26:52.966609: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-01 12:26:54.075599: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-01 12:26:54.075919: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-01 12:26:54.075935: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1609] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2022-03-01 12:26:54.076260: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-03-01 12:26:54.076323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21634 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "#glue_mnli_v002\n",
    "#super_glue_cb_v102\n",
    "dataset = seqio.get_mixture_or_task(\"glue_mnli_v002\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a8f53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {\n",
    "    \"idx\": [],\n",
    "    \"inputs_pretokenized\": [],\n",
    "    \"targets_pretokenized\": []\n",
    "}\n",
    "for i, ex in enumerate(iterator):\n",
    "    dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "    dictionary[\"inputs_pretokenized\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\") )\n",
    "    dictionary[\"targets_pretokenized\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "559fa46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77135ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idx', 'inputs_pretokenized', 'targets_pretokenized']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad0adadd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"mnli hypothesis: You won't learn anything by serving overseas.  premise: because actually when you when you do uh service overseas you end up learning something usually that's that's really useful plumbing or farming or or something like that so you're really learning a skill\",\n",
       " 'mnli hypothesis: This is a stringed instrument created by the ancient Aztecs. premise: The geiro, a percussion instrument made of a notched dried gourd, was developed by the Taano Indians.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['inputs_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb347442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['contradiction', 'contradiction']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['targets_pretokenized'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75bc9be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf1b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = seqio.get_mixture_or_task(\"super_glue_boolq_v102\").get_dataset(\n",
    "    sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "    split=\"train\",\n",
    "    shuffle=True,\n",
    "    num_epochs=1,\n",
    "    shard_info=seqio.ShardInfo(index=0, num_shards=10),\n",
    "    use_cached=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "iterator = dataset.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeaa52a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69b1ab7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(name,split):\n",
    "    \n",
    "    dataset = seqio.get_mixture_or_task(name).get_dataset(\n",
    "        sequence_length={\"inputs\": 256, \"targets\": 128},\n",
    "        split=split,\n",
    "        shuffle=True,\n",
    "        num_epochs=1,\n",
    "        shard_info=seqio.ShardInfo(index=0, num_shards=1),\n",
    "        use_cached=False,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    iterator = dataset.as_numpy_iterator()\n",
    "\n",
    "    dictionary = {\n",
    "        \"idx\": [],\n",
    "        \"processed_input\": [],\n",
    "        \"processed_output\": []\n",
    "    }\n",
    "    for i, ex in enumerate(iterator):\n",
    "        dictionary[\"idx\"].append(ex[\"idx\"])\n",
    "        dictionary[\"processed_input\"].append(ex[\"inputs_pretokenized\"].decode(\"utf-8\"))\n",
    "        dictionary[\"processed_output\"].append(ex[\"targets_pretokenized\"].decode(\"utf-8\"))\n",
    "    \n",
    "    return Dataset.from_dict(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "570085b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,concatenate_datasets,DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a76f71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seqio_mnli_dataset=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75c116a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "test_matched\n",
      "test_mismatched\n",
      "validation_matched\n",
      "validation_mismatched\n"
     ]
    }
   ],
   "source": [
    "name=\"glue_mnli_v002\"\n",
    "#super_glue_cb_v102\n",
    "for split in ['train','test_matched','test_mismatched','validation_matched','validation_mismatched']:\n",
    "    print(split)\n",
    "    seqio_mnli_dataset[split]=get_date(name,split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "838d4dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c721cdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mnli hypothesis: He laughed when he talked about finding the new dog. premise: Thank you, I will. He laughed rather ruefully, as he described how he had discovered a very rare species of fern in an inaccessible place, and in his efforts to obtain it had lost his footing, and slipped ignominiously into a neighbouring pond. '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqio_mnli_dataset['train']['processed_input'][392701]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fc2f09d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation and test dataset by combining matched and mismatched\n",
    "\n",
    "seqio_mnli_dataset['validation']=concatenate_datasets([seqio_mnli_dataset['validation_matched'],seqio_mnli_dataset['validation_mismatched']])\n",
    "seqio_mnli_dataset['test']=concatenate_datasets([seqio_mnli_dataset['test_matched'],seqio_mnli_dataset['test_mismatched']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1cef7aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8fba95a5d2745e5a12c08b2d43084c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-01 12:34:14.051296: W tensorflow/core/kernels/data/cache_dataset_ops.cc:768] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "WARNING:datasets.dataset_dict:Pushing split test_matched to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9beacde39fa04f688b091fb51c36a722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test_mismatched to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a316bbb4cef469387f63ec9a9518bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation_matched to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6ad941d1f149adbf85b39bbcbd1b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation_mismatched to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d22bd106ec247f1853469fbe0382e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1d909f847b4928a0ad178917c31465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b5bfb2d10154c859476ae5b47e573c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seqio_mnli_dataset.push_to_hub(\n",
    "        \"stjokerli/TextToText_mnli_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f8a98010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcdaf13a27d4e78bb6566c6f924f3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.50k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration stjokerli--TextToText_mnli_seqio-87ed5d481a855188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 54.48 MiB, generated: 98.47 MiB, post-processed: Unknown size, total: 152.95 MiB) to /root/.cache/huggingface/datasets/parquet/stjokerli--TextToText_mnli_seqio-87ed5d481a855188/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ffa61bd7f3848d2a1402d2252576119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9144e08150405fbc0a346ee63cad2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a92d3411f9424425a41b872c934fb5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6d17e30e434dfeaedd8dc3d40605dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b217e0967cfd4c27961bcf1ee69b7363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.16M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a863e83879ae4b2bb49c38ad7cb4ba17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/47.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8419f475ccf1418ba2136dfc02e37dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d8e4f69a2143bc8c79765d2e357416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.20M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1360d83971148b59b3b003801fe4451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/stjokerli--TextToText_mnli_seqio-87ed5d481a855188/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "421879cf173d4f8a981e60a83c9c2f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 19643\n",
       "    })\n",
       "    validation_matched: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 9815\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 19647\n",
       "    })\n",
       "    test_matched: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 9796\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 392702\n",
       "    })\n",
       "    test_mismatched: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 9847\n",
       "    })\n",
       "    validation_mismatched: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 9832\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"stjokerli/TextToText_mnli_seqio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2488e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_text_SuperGlue=DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2dc47018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cb\n",
      "  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: super_glue/cb/1.0.2\n",
      "INFO:absl:Load dataset info from /tmp/tmpdy8k0haitfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Sharding at the data source: 0 of 1\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: super_glue/cb/1.0.2\n",
      "INFO:absl:Load dataset info from /tmp/tmpdnp7_z1btfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset super_glue (~/tensorflow_datasets/super_glue/cb/1.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 73.71 KiB (download: 73.71 KiB, generated: Unknown size, total: 73.71 KiB) to ~/tensorflow_datasets/super_glue/cb/1.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86e42e8a8fac4ed5a2da4af7f60a119b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81eb8ab80c9046e085dbafe5dfcc3e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627241c5ec11464da73821c5594ee3df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/glue/superglue/data/v2/CB.zip into /root/tensorflow_datasets/downloads/dl.fbaipublicfile.com_glue_superglue_v2_CB6itp-ktvqVy3U_d97UGtqB37ewTcsPwsg74cVthM5cI.zip.tmp.583e95e8a5cf48cbbbde8c50b09d1a1f...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701cc013fe5648e79d8428a04ae486c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61cf6be959054ed9a385c526ae9dd882",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f34565145e4f64a42d263497a92f1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/cb/1.0.2.incompleteF1FWER/super_glue-train.tfrecord*...:   0%|     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/cb/1.0.2.incompleteF1FWER/super_glue-train.tfrecord*. Number of examples: 250 (shards: [250])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48adfd6d8d3b4fa981d586eb3614ea69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/56 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a20b164c728405bb6b415d79d1a5bd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/cb/1.0.2.incompleteF1FWER/super_glue-validation.tfrecord*...:   0%|…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/cb/1.0.2.incompleteF1FWER/super_glue-validation.tfrecord*. Number of examples: 56 (shards: [56])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730d7d38d89f43a788a4332c1a4b8851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e463fdd1933b4d25b14a92f81da48236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/cb/1.0.2.incompleteF1FWER/super_glue-test.tfrecord*...:   0%|      …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/cb/1.0.2.incompleteF1FWER/super_glue-test.tfrecord*. Number of examples: 250 (shards: [250])\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split train, from ~/tensorflow_datasets/super_glue/cb/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_cb_v102:train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset super_glue downloaded and prepared to ~/tensorflow_datasets/super_glue/cb/1.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Sharding at the data source: 0 of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ~/tensorflow_datasets/super_glue/cb/1.0.2\n",
      "INFO:absl:Reusing dataset super_glue (~/tensorflow_datasets/super_glue/cb/1.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split test, from ~/tensorflow_datasets/super_glue/cb/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_cb_v102:test'\n",
      "INFO:absl:Sharding at the data source: 0 of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ~/tensorflow_datasets/super_glue/cb/1.0.2\n",
      "INFO:absl:Reusing dataset super_glue (~/tensorflow_datasets/super_glue/cb/1.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split validation, from ~/tensorflow_datasets/super_glue/cb/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_cb_v102:validation'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boolq\n",
      "  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: super_glue/boolq/1.0.2\n",
      "INFO:absl:Load dataset info from /tmp/tmpeqoyaufitfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Sharding at the data source: 0 of 1\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: super_glue/boolq/1.0.2\n",
      "INFO:absl:Load dataset info from /tmp/tmpdt17etajtfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset super_glue (~/tensorflow_datasets/super_glue/boolq/1.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 3.93 MiB (download: 3.93 MiB, generated: Unknown size, total: 3.93 MiB) to ~/tensorflow_datasets/super_glue/boolq/1.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed2efb441114bddbf18d5d877d90b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3cf2a57eb514e3caa1499f3629394e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae6b427ef0e4c5aa8f755b144c0aa5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/glue/superglue/data/v2/BoolQ.zip into /root/tensorflow_datasets/downloads/dl.fbaipublicf.com_glue_superglue_v2_BoolQCy5tmUeU-X3py3LZlvIApsvmfkGGfswBSVgdYquC030.zip.tmp.35a7ddf9e76344abb2dd7d19f8566931...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bec3ad1e61814ef989b03768768cb9b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd6681b5c48141e2b3c136b4a9481dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/9427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7fd33123bc4aefa335c9401d0be2a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteYPJ0ZU/super_glue-train.tfrecord*...:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteYPJ0ZU/super_glue-train.tfrecord*. Number of examples: 9427 (shards: [9427])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f16bbc2040a433f99e9381c3923b183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/3270 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb47c46229624355b9905528f8bc7e09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteYPJ0ZU/super_glue-validation.tfrecord*...:   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteYPJ0ZU/super_glue-validation.tfrecord*. Number of examples: 3270 (shards: [3270])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f28c1267e3b54a709029ff63b5dcb5c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/3245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102c1ad4c527456fa79274fb3465b7e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteYPJ0ZU/super_glue-test.tfrecord*...:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/boolq/1.0.2.incompleteYPJ0ZU/super_glue-test.tfrecord*. Number of examples: 3245 (shards: [3245])\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split train, from ~/tensorflow_datasets/super_glue/boolq/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_boolq_v102:train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset super_glue downloaded and prepared to ~/tensorflow_datasets/super_glue/boolq/1.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Sharding at the data source: 0 of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ~/tensorflow_datasets/super_glue/boolq/1.0.2\n",
      "INFO:absl:Reusing dataset super_glue (~/tensorflow_datasets/super_glue/boolq/1.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split test, from ~/tensorflow_datasets/super_glue/boolq/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_boolq_v102:test'\n",
      "INFO:absl:Sharding at the data source: 0 of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ~/tensorflow_datasets/super_glue/boolq/1.0.2\n",
      "INFO:absl:Reusing dataset super_glue (~/tensorflow_datasets/super_glue/boolq/1.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split validation, from ~/tensorflow_datasets/super_glue/boolq/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_boolq_v102:validation'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rte\n",
      "  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: super_glue/rte/1.0.2\n",
      "INFO:absl:Load dataset info from /tmp/tmpb5enpnigtfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Sharding at the data source: 0 of 1\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: super_glue/rte/1.0.2\n",
      "INFO:absl:Load dataset info from /tmp/tmp6gft7wtetfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset super_glue (~/tensorflow_datasets/super_glue/rte/1.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 733.32 KiB (download: 733.32 KiB, generated: Unknown size, total: 733.32 KiB) to ~/tensorflow_datasets/super_glue/rte/1.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57eb380d8c61427fbdad679daac10f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40204e82782d4b898991e8f9c97110b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8cebe96da24f13a95ef6b71c01f417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/glue/superglue/data/v2/RTE.zip into /root/tensorflow_datasets/downloads/dl.fbaipublicfil.com_glue_superglue_v2_RTEuZ5Qum8w_Xht3Ep2kvtVNNXspMwVAoRqGDefP93d2qI.zip.tmp.6a4e40ee46bd4c3a8f6955b90e566548...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7b09413258f4d3f81e8bf41efea98e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a931e8b8aa9d45698112a6b67bc51075",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/2490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033f592adfa244da8e1203bf38595d05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/rte/1.0.2.incompleteCNOVHF/super_glue-train.tfrecord*...:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/rte/1.0.2.incompleteCNOVHF/super_glue-train.tfrecord*. Number of examples: 2490 (shards: [2490])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51ae1536274428cb95a15f185480ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/277 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833fdef2a2ad48d69e8764fb0996cc74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/rte/1.0.2.incompleteCNOVHF/super_glue-validation.tfrecord*...:   0%…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/rte/1.0.2.incompleteCNOVHF/super_glue-validation.tfrecord*. Number of examples: 277 (shards: [277])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db21a22d6c0f4ed28a34f6578deafbc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dabe887d8924b2bb735b327f32f800d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/rte/1.0.2.incompleteCNOVHF/super_glue-test.tfrecord*...:   0%|     …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/rte/1.0.2.incompleteCNOVHF/super_glue-test.tfrecord*. Number of examples: 3000 (shards: [3000])\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split train, from ~/tensorflow_datasets/super_glue/rte/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_rte_v102:train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset super_glue downloaded and prepared to ~/tensorflow_datasets/super_glue/rte/1.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Sharding at the data source: 0 of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ~/tensorflow_datasets/super_glue/rte/1.0.2\n",
      "INFO:absl:Reusing dataset super_glue (~/tensorflow_datasets/super_glue/rte/1.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split test, from ~/tensorflow_datasets/super_glue/rte/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_rte_v102:test'\n",
      "INFO:absl:Sharding at the data source: 0 of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ~/tensorflow_datasets/super_glue/rte/1.0.2\n",
      "INFO:absl:Reusing dataset super_glue (~/tensorflow_datasets/super_glue/rte/1.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split validation, from ~/tensorflow_datasets/super_glue/rte/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_rte_v102:validation'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copa\n",
      "  train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: super_glue/copa/1.0.2\n",
      "INFO:absl:Load dataset info from /tmp/tmp0jd6vl_8tfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Sharding at the data source: 0 of 1\n",
      "INFO:absl:Load pre-computed DatasetInfo (eg: splits, num examples,...) from GCS: super_glue/copa/1.0.2\n",
      "INFO:absl:Load dataset info from /tmp/tmpqwr99bqftfds\n",
      "INFO:absl:Field info.description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.config_description from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.citation from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.location from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.splits from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Field info.module_name from disk and from code do not match. Keeping the one from code.\n",
      "INFO:absl:Generating dataset super_glue (~/tensorflow_datasets/super_glue/copa/1.0.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 42.96 KiB (download: 42.96 KiB, generated: Unknown size, total: 42.96 KiB) to ~/tensorflow_datasets/super_glue/copa/1.0.2...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cee22467e49460eab99286a404f22c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b592134498bb4d7dbffde0e862cbe1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936f6104cda745c3aaefd009520ff57d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Downloading https://dl.fbaipublicfiles.com/glue/superglue/data/v2/COPA.zip into /root/tensorflow_datasets/downloads/dl.fbaipublicfi.com_glue_superglue_v2_COPAU9LyCyY2AxrKl_bASv72y6Se-TNEliICWt_IgJ3osDI.zip.tmp.f9d61cd3c80642cf9fadd9ba10e8c393...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5202d8f9be38430dafd4e02e5138dd86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating splits...:   0%|          | 0/3 [00:00<?, ? splits/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6bc82ce5194c8cbfe6682b21be693c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train examples...:   0%|          | 0/400 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e06b8b04f0451b993098adabdf7f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/copa/1.0.2.incompleteM9II1E/super_glue-train.tfrecord*...:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/copa/1.0.2.incompleteM9II1E/super_glue-train.tfrecord*. Number of examples: 400 (shards: [400])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e685acb6526411bb79f95f94a86b076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation examples...:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd95e6c952494d96847696a3f15846ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/copa/1.0.2.incompleteM9II1E/super_glue-validation.tfrecord*...:   0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/copa/1.0.2.incompleteM9II1E/super_glue-validation.tfrecord*. Number of examples: 100 (shards: [100])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c18b61f0a33a4f2da8e13b9424963c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test examples...:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4302b16b99ec43a98d55f74a89a2dafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Shuffling ~/tensorflow_datasets/super_glue/copa/1.0.2.incompleteM9II1E/super_glue-test.tfrecord*...:   0%|    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Done writing ~/tensorflow_datasets/super_glue/copa/1.0.2.incompleteM9II1E/super_glue-test.tfrecord*. Number of examples: 500 (shards: [500])\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split train, from ~/tensorflow_datasets/super_glue/copa/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_copa_v102:train'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDataset super_glue downloaded and prepared to ~/tensorflow_datasets/super_glue/copa/1.0.2. Subsequent calls will reuse this data.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Sharding at the data source: 0 of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ~/tensorflow_datasets/super_glue/copa/1.0.2\n",
      "INFO:absl:Reusing dataset super_glue (~/tensorflow_datasets/super_glue/copa/1.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split test, from ~/tensorflow_datasets/super_glue/copa/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_copa_v102:test'\n",
      "INFO:absl:Sharding at the data source: 0 of 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Load dataset info from ~/tensorflow_datasets/super_glue/copa/1.0.2\n",
      "INFO:absl:Reusing dataset super_glue (~/tensorflow_datasets/super_glue/copa/1.0.2)\n",
      "INFO:absl:Constructing tf.data.Dataset super_glue for split validation, from ~/tensorflow_datasets/super_glue/copa/1.0.2\n",
      "INFO:absl:Automatically caching small dataset in memory: 'super_glue_copa_v102:validation'\n"
     ]
    }
   ],
   "source": [
    "# task_dict=\n",
    "for name in ['cb','boolq','rte','copa']:\n",
    "    seqio_name=f\"super_glue_{name}_v102\"\n",
    "    print(name)\n",
    "    temp=DatasetDict()\n",
    "    for split in ['train','test','validation']:\n",
    "        \n",
    "        print(\" \",split)\n",
    "        temp[split]=get_date(seqio_name,split)\n",
    "    text_to_text_SuperGlue[name]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae79c4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    cb: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 250\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 56\n",
       "        })\n",
       "    })\n",
       "    boolq: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 9427\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 3245\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 3270\n",
       "        })\n",
       "    })\n",
       "    rte: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 2490\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 3000\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 277\n",
       "        })\n",
       "    })\n",
       "    copa: DatasetDict({\n",
       "        train: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 400\n",
       "        })\n",
       "        test: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 500\n",
       "        })\n",
       "        validation: Dataset({\n",
       "            features: ['idx', 'processed_input', 'processed_output'],\n",
       "            num_rows: 100\n",
       "        })\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_text_SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe1411f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc109535f5804443ad98d090fed77d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2791482502be4b98bf3355fda5759254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8f94e003fd74152ada632d584388ac6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c140eb655db43029b7827861b42e902",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3bfc700e134aafa586cb298fe78cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdedcb9f0095464b94ebd725fa93fef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efda33542d25455586bc67a0d265cd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b710db82b22f4ccda3c15ed45883737c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2d40b8dd0e840c58654047fc79dba2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split train to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bf8744ed0d0427f97d43c6addb5c765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split test to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1b9aaf37ac4e1db021bfa6c32e1c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.dataset_dict:Pushing split validation to the Hub.\n",
      "WARNING:datasets.arrow_dataset:The repository already exists: the `private` keyword argument will be ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e7a5ff3617a41f597a79a1f4dba10e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for task in text_to_text_SuperGlue.keys():\n",
    "    text_to_text_SuperGlue[task].push_to_hub(\n",
    "        f\"stjokerli/TextToText_{task}_seqio\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25fd3fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d5134ac4fe44168affdea3ab8dbd3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/922 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration stjokerli--TextToText_cb_seqio-6ec7039b16c98d10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None (download: 126.38 KiB, generated: 211.48 KiB, post-processed: Unknown size, total: 337.86 KiB) to /root/.cache/huggingface/datasets/parquet/stjokerli--TextToText_cb_seqio-6ec7039b16c98d10/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9461fdb77654498697a34b700f273eb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4612d51cbf148828979e3165484cd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/54.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bd5720c3584417b5608789f97206d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/57.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf6998226d8a49b186775f025387346c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/17.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf052844b4b45d8bd22d2bf8f4f118b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/parquet/stjokerli--TextToText_cb_seqio-6ec7039b16c98d10/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39eae8d3c984d5eb5db72fdd68bd0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 250\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['idx', 'processed_input', 'processed_output'],\n",
       "        num_rows: 56\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"stjokerli/TextToText_cb_seqio\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a34f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
