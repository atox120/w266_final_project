{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701a85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import seqio\n",
    "import t5.data\n",
    "from t5.data import postprocessors\n",
    "from t5.data import preprocessors\n",
    "from t5.data.glue_utils import get_glue_metric\n",
    "from t5.data.glue_utils import get_glue_postprocess_fn\n",
    "from t5.data.glue_utils import get_glue_text_preprocessor\n",
    "from t5.data.glue_utils import get_super_glue_metric\n",
    "from t5.evaluation import metrics\n",
    "import tensorflow_datasets as tfds\n",
    "from t5.models import utils as model_utils\n",
    "import gin\n",
    "from absl import logging\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24012f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqio\n",
    "from transformers import BartTokenizer\n",
    "#>>> tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "#    >>> tokenizer(\"Hello world\")['input_ids']\n",
    "#    [0, 31414, 232, 2]\n",
    "#    >>> tokenizer(\" Hello world\")['input_ids']\n",
    "#    [0, 20920, 232, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6daf9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNLI_args = {\n",
    "#    \"tfds_name\" = \"MultiNLI\",\n",
    "#    \"version\" = \"1.1.0\"\n",
    "#}\n",
    "\n",
    "task_name = \"MNLI\"\n",
    "task_version = \"1.1.0\"\n",
    "\n",
    "# =================================== Squad ====================================\n",
    "# Maximized evaluation metrics over all answers.\n",
    "squad_v110_allanswers = {\n",
    "  \"tfds_name\": \"squad/v1.1:3.0.0\",\n",
    "  \"model\": \"Mustang\",\n",
    "  \"year\": 1964\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example from tfds\n",
    "\n",
    "seqio.TaskRegistry.add(\n",
    "    f\"{task_name}_{task_version}\",\n",
    "    seqio.TfdsDataSource(tfds_name=\"MultiNLI:1.1.0\"),\n",
    "    preprocessors=[translate, seqio.preprocessors.tokenize, seqio.preprocessors.append_eos],\n",
    "    output_features={\n",
    "        \"inputs\": seqio.Feature(\n",
    "            #Use if specifying vocabulary to specify how the feature can be tokenized/detokenized.\n",
    "            #seqio.SentencePieceVocabulary(\"/path/to/inputs/vocab\"),\n",
    "            seqio.PassThroughVocabulary,\n",
    "            add_eos=False, dtype=tf.int32\n",
    "        ),\n",
    "        \"targets\": seqio.Feature(\n",
    "            #Use if specifying vocabulary to specify how the feature can be tokenized/detokenized.\n",
    "            #seqio.SentencePieceVocabulary(\"/path/to/inputs/vocab\"),\n",
    "            seqio.PassThroughVocabulary,\n",
    "            add_eos=True, dtype=tf.int32\n",
    "        ),\n",
    "    },\n",
    "    metric_fns=[bleu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f7cba9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'translate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Example from tfds\u001b[39;00m\n\u001b[1;32m      3\u001b[0m seqio\u001b[38;5;241m.\u001b[39mTaskRegistry\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     seqio\u001b[38;5;241m.\u001b[39mTfdsDataSource(tfds_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMultiNLI:1.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m----> 6\u001b[0m     preprocessors\u001b[38;5;241m=\u001b[39m[\u001b[43mtranslate\u001b[49m, seqio\u001b[38;5;241m.\u001b[39mpreprocessors\u001b[38;5;241m.\u001b[39mtokenize, seqio\u001b[38;5;241m.\u001b[39mpreprocessors\u001b[38;5;241m.\u001b[39mappend_eos],\n\u001b[1;32m      7\u001b[0m     output_features\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: seqio\u001b[38;5;241m.\u001b[39mFeature(\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;66;03m#Use if specifying vocabulary to specify how the feature can be tokenized/detokenized.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;66;03m#seqio.SentencePieceVocabulary(\"/path/to/inputs/vocab\"),\u001b[39;00m\n\u001b[1;32m     11\u001b[0m             seqio\u001b[38;5;241m.\u001b[39mPassThroughVocabulary,\n\u001b[1;32m     12\u001b[0m             add_eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32\n\u001b[1;32m     13\u001b[0m         ),\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m: seqio\u001b[38;5;241m.\u001b[39mFeature(\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;66;03m#Use if specifying vocabulary to specify how the feature can be tokenized/detokenized.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;66;03m#seqio.SentencePieceVocabulary(\"/path/to/inputs/vocab\"),\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             seqio\u001b[38;5;241m.\u001b[39mPassThroughVocabulary,\n\u001b[1;32m     18\u001b[0m             add_eos\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32\n\u001b[1;32m     19\u001b[0m         ),\n\u001b[1;32m     20\u001b[0m     },\n\u001b[1;32m     21\u001b[0m     metric_fns\u001b[38;5;241m=\u001b[39m[bleu])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'translate' is not defined"
     ]
    }
   ],
   "source": [
    "### Example from tfds\n",
    "\n",
    "seqio.TaskRegistry.add(\n",
    "    f\"{task_name}_{task_version}\",\n",
    "    seqio.TfdsDataSource(tfds_name=\"MultiNLI:1.1.0\"),\n",
    "    preprocessors=[translate, seqio.preprocessors.tokenize, seqio.preprocessors.append_eos],\n",
    "    output_features={\n",
    "        \"inputs\": seqio.Feature(\n",
    "            #Use if specifying vocabulary to specify how the feature can be tokenized/detokenized.\n",
    "            #seqio.SentencePieceVocabulary(\"/path/to/inputs/vocab\"),\n",
    "            seqio.PassThroughVocabulary,\n",
    "            add_eos=False, dtype=tf.int32\n",
    "        ),\n",
    "        \"targets\": seqio.Feature(\n",
    "            #Use if specifying vocabulary to specify how the feature can be tokenized/detokenized.\n",
    "            #seqio.SentencePieceVocabulary(\"/path/to/inputs/vocab\"),\n",
    "            seqio.PassThroughVocabulary,\n",
    "            add_eos=True, dtype=tf.int32\n",
    "        ),\n",
    "    },\n",
    "    metric_fns=[bleu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90afcb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "seqio.TaskRegistry.add(\n",
    "    \"wmt19_ende\",\n",
    "    seqio.TfdsDataSource(tfds_name=\"wmt19_translate/de-en:1.0.0\"),\n",
    "    preprocessors=[\n",
    "        translate, seqio.preprocessors.tokenize, seqio.preprocessors.append_eos\n",
    "    ],\n",
    "    output_features={\n",
    "        \"inputs\": seqio.Feature(\n",
    "           seqio.SentencePieceVocabulary(\"/path/to/inputs/vocab\"),\n",
    "           add_eos=False, dtype=tf.int32\n",
    "        ),\n",
    "        \"targets\": seqio.Feature(\n",
    "           seqio.SentencePieceVocabulary(\"/path/to/targets/vocab\"),\n",
    "           add_eos=True, dtype=tf.int32\n",
    "        ),\n",
    "    },\n",
    "    metric_fns=[bleu])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb56d175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datasets task\n",
    "\n",
    "class task:\n",
    "    def __init__(self):\n",
    "        \n",
    "        #Create a new task registry. \n",
    "        self.TaskRegistry = seqio.TaskRegistry\n",
    "        \n",
    "        #Define Default output task schema\n",
    "        self.DEFAULT_OUTPUT_FEATURES = {\n",
    "            \"inputs\": seqio.Feature(\n",
    "                vocabulary=t5.data.get_default_vocabulary(), add_eos=True,\n",
    "                required=False),\n",
    "            \"targets\": seqio.Feature(\n",
    "                vocabulary=t5.data.get_default_vocabulary(), add_eos=True)\n",
    "}\n",
    "        \n",
    "# ==================================== C4 ======================================\n",
    "        # Final pretraining task used in Raffel et al., 2019.\n",
    "        self.TaskRegistry.add(\n",
    "            \"c4_v220_span_corruption\",\n",
    "            source=seqio.TfdsDataSource(tfds_name=\"c4/en:2.2.0\"),\n",
    "            preprocessors=[\n",
    "                functools.partial(\n",
    "                    preprocessors.rekey, key_map={\n",
    "                        \"inputs\": None,\n",
    "                        \"targets\": \"text\"\n",
    "                    }),\n",
    "                seqio.preprocessors.tokenize,\n",
    "                seqio.CacheDatasetPlaceholder(),\n",
    "                preprocessors.span_corruption,\n",
    "                seqio.preprocessors.append_eos_after_trim,\n",
    "\n",
    "            ],\n",
    "            output_features=self.DEFAULT_OUTPUT_FEATURES,\n",
    "            metric_fns=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa2c35dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mynewtask = task()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "458e339c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['c4_v220_span_corruption'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mynewtask.TaskRegistry._REGISTRY.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dea1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
